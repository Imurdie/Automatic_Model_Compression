import torch.nn as nn
import torch.nn.functional as F
import torch

class BidirectionalLSTM(nn.Module):
    # Inputs hidden units Out
    def __init__(self, nIn, nHidden, nOut):
        super(BidirectionalLSTM, self).__init__()

        self.rnn = nn.LSTM(nIn, nHidden, bidirectional=True)
        self.embedding = nn.Linear(nHidden * 2, nOut)

    def forward(self, input):
        recurrent, _ = self.rnn(input)
        T, b, h = recurrent.size()
        t_rec = recurrent.view(T * b, h)

        output = self.embedding(t_rec)  # [T * b, nOut]
        output = output.view(T, b, -1)

        return output


class CRNN(nn.Module):
    # def __init__(self, imgH, nc, nclass, nh, n_rnn=2, leakyRelu=False, phi=0):
    def __init__(self, imgH, nc, nclass, nh, n_rnn=2, leakyRelu=False):
        super(CRNN, self).__init__()
        # assert imgH % 16 == 0, 'imgH has to be a multiple of 16'

        ks = [3, 3, 3, 3, 3, 3, 3]
        ps = [1, 1, 1, 1, 1, 1, 0]
        ss = [1, 1, 1, 1, 1, 1, (3, 1)]
        nm = [64, 128, 256, 256, 512, 512, 512]

        cnn = nn.Sequential()

        # if phi>=1 and phi<=3:
        #   self

        def convRelu(i, batchNormalization=False):
            nIn = nc if i == 0 else nm[i - 1]
            nOut = nm[i]
            cnn.add_module('conv{0}'.format(i),
                           nn.Conv2d(nIn, nOut, ks[i], ss[i], ps[i]))
            if batchNormalization:
                cnn.add_module('batchnorm{0}'.format(i), nn.BatchNorm2d(nOut))
            if leakyRelu:
                cnn.add_module('relu{0}'.format(i),
                               nn.LeakyReLU(0.2, inplace=True))
            else:
                cnn.add_module('relu{0}'.format(i), nn.ReLU(True))

        convRelu(0)
        cnn.add_module('pooling{0}'.format(0), nn.MaxPool2d(2, 2))  # 64*100*100
        convRelu(1)
        cnn.add_module('pooling{0}'.format(1), nn.MaxPool2d(2, 2))  # 128*50*50
        convRelu(2, True)
        convRelu(3)
        cnn.add_module('pooling{0}'.format(2), nn.MaxPool2d(2, 2))  # 256x25x25
        convRelu(4, True)
        convRelu(5)

        # cnn.add_module('pooling{0}'.format(3), nn.MaxPool2d(2, 2))  # 512*8*11

        cnn.add_module('pooling{0}'.format(3),
                       nn.MaxPool2d((3, 3), (3, 2), (1, 0)))  # 512*20*20

        convRelu(6)  # 512*2*10
        # self.Relu6 = convRelu(6)

        cnn.add_module('pooling{0}'.format(4),
                       nn.MaxPool2d((2, 3), (3, 1), (0, 1)))  # 512*1*10

        # #                            # 512*6*6=18432

        # self.flatten = nn.Flatten()    #32*18432

        # self.fc0 = nn.Linear(18432, 2048)
        # self.Relu7 = nn.ReLU(inplace=True)
        # self.drop0 = nn.Dropout(p=0.5)
        # self.fc1 = nn.Linear(2048, 2048)
        # self.Relu8 = nn.ReLU(inplace=True)

        # self.to_lstm = nn.Sequential(

        #     nn.ReLU(inplace=true),
        #     nn.Linear(2048, 10)
        # )

        self.cnn = cnn
        self.rnn = nn.Sequential(
            BidirectionalLSTM(512, nh, nh),
            BidirectionalLSTM(nh, nh, nclass))
        self.flatten = nn.Flatten()  # 32*100

        self.fc0 = nn.Linear(100, 10)
        self.Relu7 = nn.ReLU(inplace=True)
        # self.drop0 = nn.Dropout(p=0.5)
        self.fc1 = nn.Linear(2048, 10)
        # self.Relu8 = nn.ReLU(inplace=True)
        # self.softmax = nn.Softmax(dim=0)

    def forward(self, input):

        # conv features
        conv = self.cnn(input)  # [32, 512, 1, 10]
        b, c, h, w = conv.size()

        # print('conv:', conv.size())

        assert h == 1, "the height of conv must be 1"
        conv = conv.squeeze(2)  # b *512 * width
        conv = conv.permute(2, 0, 1)  # [w, b, c]

        rnn_out = self.rnn(conv)  # [10, 32, 4]   new[10, 32, 10]
        # print('rnn_out:', rnn_out.size())

        rnn_out = rnn_out.permute(1, 0, 2)  # [b, w, c]
        y = self.flatten(rnn_out)  # [b, w*c]
        # print('flatten:', y.size())
        output = self.fc0(y)
        # print('fc0:', output.size())
     

        return output

model_crnn=CRNN(200,1,10,64)

# model_crnn(images[0]).shape

{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Imurdie/Automatic_Model_Compression/blob/main/AMC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B0QV_B2_I9HO",
        "outputId": "71877d6a-0883-4ad8-f3f6-80a8118b0517"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sun Mar  5 14:13:06 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   51C    P0    28W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lXvl4-e9I7pL",
        "outputId": "55740558-aabb-42fe-cc38-4adc6482773e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/amc\n"
          ]
        }
      ],
      "source": [
        "%cd '/content/amc'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hSzw7rfgJpio",
        "outputId": "00ffaff1-22c8-414a-91cd-6c1f32295c71"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'amc'...\n",
            "remote: Enumerating objects: 73, done.\u001b[K\n",
            "remote: Total 73 (delta 0), reused 0 (delta 0), pack-reused 73\u001b[K\n",
            "Unpacking objects: 100% (73/73), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/mit-han-lab/amc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PmayxThvKtH_",
        "outputId": "8cf81dfd-0e0a-41d1-c986-b717b83c4a3c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ],
      "source": [
        "!pwd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W2v0fQm8-jpN"
      },
      "source": [
        "##too old to run"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YcQ88Vz8KABI",
        "outputId": "e6a67f89-aafd-4ce1-c4d0-0e72c490cd58"
      },
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-0a8c6f3b8589>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackends\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcudnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeterministic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchannel_pruning_env\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mChannelPruningEnv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magent\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDDPG\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_output_folder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/amc/env/channel_pruning_env.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAverageMeter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprGreen\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_split_dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrewards\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/amc/lib/utils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[0;31m# Custom progress bar\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mterm_width\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'stty size'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    172\u001b[0m \u001b[0mterm_width\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mterm_width\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[0mTOTAL_BAR_LENGTH\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m40.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 2, got 0)"
          ]
        }
      ],
      "source": [
        "# Code for \"AMC: AutoML for Model Compression and Acceleration on Mobile Devices\"\n",
        "# Yihui He*, Ji Lin*, Zhijian Liu, Hanrui Wang, Li-Jia Li, Song Han\n",
        "# {jilin, songhan}@mit.edu\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import argparse\n",
        "from copy import deepcopy\n",
        "import torch\n",
        "torch.backends.cudnn.deterministic = True\n",
        "\n",
        "from env.channel_pruning_env import ChannelPruningEnv\n",
        "from lib.agent import DDPG\n",
        "from lib.utils import get_output_folder\n",
        "\n",
        "from tensorboardX import SummaryWriter\n",
        "\n",
        "\n",
        "def parse_args():\n",
        "    parser = argparse.ArgumentParser(description='AMC search script')\n",
        "\n",
        "    parser.add_argument('--job', default='train', type=str, help='support option: train/export')\n",
        "    parser.add_argument('--suffix', default=None, type=str, help='suffix to help you remember what experiment you ran')\n",
        "    # env\n",
        "    parser.add_argument('--model', default='mobilenet', type=str, help='model to prune')\n",
        "    parser.add_argument('--dataset', default='imagenet', type=str, help='dataset to use (cifar/imagenet)')\n",
        "    parser.add_argument('--data_root', default=None, type=str, help='dataset path')\n",
        "    parser.add_argument('--preserve_ratio', default=0.5, type=float, help='preserve ratio of the model')\n",
        "    parser.add_argument('--lbound', default=0.2, type=float, help='minimum preserve ratio')\n",
        "    parser.add_argument('--rbound', default=1., type=float, help='maximum preserve ratio')\n",
        "    parser.add_argument('--reward', default='acc_reward', type=str, help='Setting the reward')\n",
        "    parser.add_argument('--acc_metric', default='acc5', type=str, help='use acc1 or acc5')\n",
        "    parser.add_argument('--use_real_val', dest='use_real_val', action='store_true')\n",
        "    parser.add_argument('--ckpt_path', default=None, type=str, help='manual path of checkpoint')\n",
        "    # parser.add_argument('--pruning_method', default='cp', type=str,\n",
        "    #                     help='method to prune (fg/cp for fine-grained and channel pruning)')\n",
        "    # only for channel pruning\n",
        "    parser.add_argument('--n_calibration_batches', default=60, type=int,\n",
        "                        help='n_calibration_batches')\n",
        "    parser.add_argument('--n_points_per_layer', default=10, type=int,\n",
        "                        help='method to prune (fg/cp for fine-grained and channel pruning)')\n",
        "    parser.add_argument('--channel_round', default=8, type=int, help='Round channel to multiple of channel_round')\n",
        "    # ddpg\n",
        "    parser.add_argument('--hidden1', default=300, type=int, help='hidden num of first fully connect layer')\n",
        "    parser.add_argument('--hidden2', default=300, type=int, help='hidden num of second fully connect layer')\n",
        "    parser.add_argument('--lr_c', default=1e-3, type=float, help='learning rate for actor')\n",
        "    parser.add_argument('--lr_a', default=1e-4, type=float, help='learning rate for actor')\n",
        "    parser.add_argument('--warmup', default=100, type=int,\n",
        "                        help='time without training but only filling the replay memory')\n",
        "    parser.add_argument('--discount', default=1., type=float, help='')\n",
        "    parser.add_argument('--bsize', default=64, type=int, help='minibatch size')\n",
        "    parser.add_argument('--rmsize', default=100, type=int, help='memory size for each layer')\n",
        "    parser.add_argument('--window_length', default=1, type=int, help='')\n",
        "    parser.add_argument('--tau', default=0.01, type=float, help='moving average for target network')\n",
        "    # noise (truncated normal distribution)\n",
        "    parser.add_argument('--init_delta', default=0.5, type=float,\n",
        "                        help='initial variance of truncated normal distribution')\n",
        "    parser.add_argument('--delta_decay', default=0.95, type=float,\n",
        "                        help='delta decay during exploration')\n",
        "    # training\n",
        "    parser.add_argument('--max_episode_length', default=1e9, type=int, help='')\n",
        "    parser.add_argument('--output', default='./logs', type=str, help='')\n",
        "    parser.add_argument('--debug', dest='debug', action='store_true')\n",
        "    parser.add_argument('--init_w', default=0.003, type=float, help='')\n",
        "    parser.add_argument('--train_episode', default=800, type=int, help='train iters each timestep')\n",
        "    parser.add_argument('--epsilon', default=50000, type=int, help='linear decay of exploration policy')\n",
        "    parser.add_argument('--seed', default=None, type=int, help='random seed to set')\n",
        "    parser.add_argument('--n_gpu', default=1, type=int, help='number of gpu to use')\n",
        "    parser.add_argument('--n_worker', default=16, type=int, help='number of data loader worker')\n",
        "    parser.add_argument('--data_bsize', default=50, type=int, help='number of data batch size')\n",
        "    parser.add_argument('--resume', default='default', type=str, help='Resuming model path for testing')\n",
        "    # export\n",
        "    parser.add_argument('--ratios', default=None, type=str, help='ratios for pruning')\n",
        "    parser.add_argument('--channels', default=None, type=str, help='channels after pruning')\n",
        "    parser.add_argument('--export_path', default=None, type=str, help='path for exporting models')\n",
        "    parser.add_argument('--use_new_input', dest='use_new_input', action='store_true', help='use new input feature')\n",
        "\n",
        "    return parser.parse_args()\n",
        "\n",
        "\n",
        "def get_model_and_checkpoint(model, dataset, checkpoint_path, n_gpu=1):\n",
        "    if model == 'mobilenet' and dataset == 'imagenet':\n",
        "        from models.mobilenet import MobileNet\n",
        "        net = MobileNet(n_class=1000)\n",
        "    elif model == 'mobilenetv2' and dataset == 'imagenet':\n",
        "        from models.mobilenet_v2 import MobileNetV2\n",
        "        net = MobileNetV2(n_class=1000)\n",
        "    else:\n",
        "        raise NotImplementedError\n",
        "    sd = torch.load(checkpoint_path)\n",
        "    if 'state_dict' in sd:  # a checkpoint but not a state_dict\n",
        "        sd = sd['state_dict']\n",
        "    sd = {k.replace('module.', ''): v for k, v in sd.items()}\n",
        "    net.load_state_dict(sd)\n",
        "    net = net.cuda()\n",
        "    if n_gpu > 1:\n",
        "        net = torch.nn.DataParallel(net, range(n_gpu))\n",
        "\n",
        "    return net, deepcopy(net.state_dict())\n",
        "\n",
        "\n",
        "def train(num_episode, agent, env, output):\n",
        "    agent.is_training = True\n",
        "    step = episode = episode_steps = 0\n",
        "    episode_reward = 0.\n",
        "    observation = None\n",
        "    T = []  # trajectory\n",
        "    while episode < num_episode:  # counting based on episode\n",
        "        # reset if it is the start of episode\n",
        "        if observation is None:\n",
        "            observation = deepcopy(env.reset())\n",
        "            agent.reset(observation)\n",
        "\n",
        "        # agent pick action ...\n",
        "        if episode <= args.warmup:\n",
        "            action = agent.random_action()\n",
        "            # action = sample_from_truncated_normal_distribution(lower=0., upper=1., mu=env.preserve_ratio, sigma=0.5)\n",
        "        else:\n",
        "            action = agent.select_action(observation, episode=episode)\n",
        "\n",
        "        # env response with next_observation, reward, terminate_info\n",
        "        observation2, reward, done, info = env.step(action)\n",
        "        observation2 = deepcopy(observation2)\n",
        "\n",
        "        T.append([reward, deepcopy(observation), deepcopy(observation2), action, done])\n",
        "\n",
        "        # fix-length, never reach here\n",
        "        # if max_episode_length and episode_steps >= max_episode_length - 1:\n",
        "        #     done = True\n",
        "\n",
        "        # [optional] save intermideate model\n",
        "        if episode % int(num_episode / 3) == 0:\n",
        "            agent.save_model(output)\n",
        "\n",
        "        # update\n",
        "        step += 1\n",
        "        episode_steps += 1\n",
        "        episode_reward += reward\n",
        "        observation = deepcopy(observation2)\n",
        "\n",
        "        if done:  # end of episode\n",
        "            print('#{}: episode_reward:{:.4f} acc: {:.4f}, ratio: {:.4f}'.format(episode, episode_reward,\n",
        "                                                                                 info['accuracy'],\n",
        "                                                                                 info['compress_ratio']))\n",
        "            text_writer.write(\n",
        "                '#{}: episode_reward:{:.4f} acc: {:.4f}, ratio: {:.4f}\\n'.format(episode, episode_reward,\n",
        "                                                                                 info['accuracy'],\n",
        "                                                                                 info['compress_ratio']))\n",
        "            final_reward = T[-1][0]\n",
        "            # print('final_reward: {}'.format(final_reward))\n",
        "            # agent observe and update policy\n",
        "            for r_t, s_t, s_t1, a_t, done in T:\n",
        "                agent.observe(final_reward, s_t, s_t1, a_t, done)\n",
        "                if episode > args.warmup:\n",
        "                    agent.update_policy()\n",
        "\n",
        "            #agent.memory.append(\n",
        "            #    observation,\n",
        "            #    agent.select_action(observation, episode=episode),\n",
        "            #    0., False\n",
        "            #)\n",
        "\n",
        "            # reset\n",
        "            observation = None\n",
        "            episode_steps = 0\n",
        "            episode_reward = 0.\n",
        "            episode += 1\n",
        "            T = []\n",
        "\n",
        "            tfwriter.add_scalar('reward/last', final_reward, episode)\n",
        "            tfwriter.add_scalar('reward/best', env.best_reward, episode)\n",
        "            tfwriter.add_scalar('info/accuracy', info['accuracy'], episode)\n",
        "            tfwriter.add_scalar('info/compress_ratio', info['compress_ratio'], episode)\n",
        "            tfwriter.add_text('info/best_policy', str(env.best_strategy), episode)\n",
        "            # record the preserve rate for each layer\n",
        "            for i, preserve_rate in enumerate(env.strategy):\n",
        "                tfwriter.add_scalar('preserve_rate/{}'.format(i), preserve_rate, episode)\n",
        "\n",
        "            text_writer.write('best reward: {}\\n'.format(env.best_reward))\n",
        "            text_writer.write('best policy: {}\\n'.format(env.best_strategy))\n",
        "    text_writer.close()\n",
        "\n",
        "\n",
        "def export_model(env, args):\n",
        "    assert args.ratios is not None or args.channels is not None, 'Please provide a valid ratio list or pruned channels'\n",
        "    assert args.export_path is not None, 'Please provide a valid export path'\n",
        "    env.set_export_path(args.export_path)\n",
        "\n",
        "    print('=> Original model channels: {}'.format(env.org_channels))\n",
        "    if args.ratios:\n",
        "        ratios = args.ratios.split(',')\n",
        "        ratios = [float(r) for r in ratios]\n",
        "        assert  len(ratios) == len(env.org_channels)\n",
        "        channels = [int(r * c) for r, c in zip(ratios, env.org_channels)]\n",
        "    else:\n",
        "        channels = args.channels.split(',')\n",
        "        channels = [int(r) for r in channels]\n",
        "        ratios = [c2 / c1 for c2, c1 in zip(channels, env.org_channels)]\n",
        "    print('=> Pruning with ratios: {}'.format(ratios))\n",
        "    print('=> Channels after pruning: {}'.format(channels))\n",
        "\n",
        "    for r in ratios:\n",
        "        env.step(r)\n",
        "\n",
        "    return\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    args = parse_args()\n",
        "\n",
        "    if args.seed is not None:\n",
        "        np.random.seed(args.seed)\n",
        "        torch.manual_seed(args.seed)\n",
        "        torch.cuda.manual_seed(args.seed)\n",
        "\n",
        "    model, checkpoint = get_model_and_checkpoint(args.model, args.dataset, checkpoint_path=args.ckpt_path,\n",
        "                                                 n_gpu=args.n_gpu)\n",
        "\n",
        "    env = ChannelPruningEnv(model, checkpoint, args.dataset,\n",
        "                            preserve_ratio=1. if args.job == 'export' else args.preserve_ratio,\n",
        "                            n_data_worker=args.n_worker, batch_size=args.data_bsize,\n",
        "                            args=args, export_model=args.job == 'export', use_new_input=args.use_new_input)\n",
        "\n",
        "    if args.job == 'train':\n",
        "        # build folder and logs\n",
        "        base_folder_name = '{}_{}_r{}_search'.format(args.model, args.dataset, args.preserve_ratio)\n",
        "        if args.suffix is not None:\n",
        "            base_folder_name = base_folder_name + '_' + args.suffix\n",
        "        args.output = get_output_folder(args.output, base_folder_name)\n",
        "        print('=> Saving logs to {}'.format(args.output))\n",
        "        tfwriter = SummaryWriter(logdir=args.output)\n",
        "        text_writer = open(os.path.join(args.output, 'log.txt'), 'w')\n",
        "        print('=> Output path: {}...'.format(args.output))\n",
        "\n",
        "        nb_states = env.layer_embedding.shape[1]\n",
        "        nb_actions = 1  # just 1 action here\n",
        "\n",
        "        args.rmsize = args.rmsize * len(env.prunable_idx)  # for each layer\n",
        "        print('** Actual replay buffer size: {}'.format(args.rmsize))\n",
        "\n",
        "        agent = DDPG(nb_states, nb_actions, args)\n",
        "        train(args.train_episode, agent, env, args.output)\n",
        "    elif args.job == 'export':\n",
        "        export_model(env, args)\n",
        "    else:\n",
        "        raise RuntimeError('Undefined job {}'.format(args.job))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hApdkxL4-0n6"
      },
      "source": [
        "##torch-pruing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xZOkUOlFoMDv",
        "outputId": "8917b973-13d7-42f8-b359-11866e86f81e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Model-Compression'...\n",
            "remote: Enumerating objects: 41, done.\u001b[K\n",
            "remote: Counting objects: 100% (25/25), done.\u001b[K\n",
            "remote: Compressing objects: 100% (18/18), done.\u001b[K\n",
            "remote: Total 41 (delta 14), reused 18 (delta 7), pack-reused 16\u001b[K\n",
            "Unpacking objects: 100% (41/41), 105.87 MiB | 10.38 MiB/s, done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/0601p/Model-Compression.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_0x5dNcSox4Y",
        "outputId": "82c8095a-0376-46f2-e6c3-0372f0cd98e2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting torch-pruning\n",
            "  Downloading torch_pruning-1.0.0-py3-none-any.whl (30 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.8/dist-packages (from torch-pruning) (1.13.1+cu116)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from torch-pruning) (1.22.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch->torch-pruning) (4.5.0)\n",
            "Installing collected packages: torch-pruning\n",
            "Successfully installed torch-pruning-1.0.0\n"
          ]
        }
      ],
      "source": [
        "!pip install torch-pruning # v1.0.0"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ejEpecJ-6jk1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "34Td-ay16s1L"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "train_filePath = '/content/12kDriveEnd_img0.2/train'\n",
        "train_file_path_list_ori = os.listdir(train_filePath)\n",
        "val_filePath = '/content/12kDriveEnd_img0.2/val'\n",
        "val_file_path_list_ori = os.listdir(val_filePath)\n",
        "test_filePath = '/content/12kDriveEnd_img0.2/test'\n",
        "test_file_path_list_ori = os.listdir(test_filePath)\n",
        "\n",
        "# file_path_list_ori[:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "3tbXlzAd6s1L"
      },
      "outputs": [],
      "source": [
        "train_file_name_list = list(set([file_path.split('-')[0] for file_path in train_file_path_list_ori]))\n",
        "val_file_name_list = list(set([file_path.split('-')[0] for file_path in val_file_path_list_ori]))\n",
        "test_file_name_list = list(set([file_path.split('-')[0] for file_path in test_file_path_list_ori]))\n",
        "# file_name_list[:5],len(file_name_list)\n",
        "# file_name_list,len(file_name_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "31b77962-6d2d-4002-b8ce-07e29863512a",
        "id": "8szDUgxG64pO"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "def get_label_num(file_name):\n",
        "  if 'B007' in file_name:\n",
        "    return 3\n",
        "  elif 'B014' in file_name:\n",
        "    return 6\n",
        "  elif 'B021' in file_name:\n",
        "    return 9\n",
        "  elif 'OR007' in file_name:\n",
        "    return 1\n",
        "  elif 'OR014' in file_name:\n",
        "    return 4\n",
        "  elif 'OR021' in file_name:\n",
        "    return 7\n",
        "  elif 'IR007' in file_name:\n",
        "    return 2\n",
        "  elif 'IR014' in file_name:\n",
        "    return 5\n",
        "  elif 'IR021' in file_name:\n",
        "    return 8\n",
        "  else :\n",
        "    return 0\n",
        "get_label_num('OR1007@12_3_60568')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "UQR2yp9X64pO"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler\n",
        "import torch.backends.cudnn as cudnn\n",
        "import numpy as np\n",
        "import torchvision\n",
        "from torchvision import datasets, models, transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7cf3c8c8-cad4-40a8-eeed-e58f1aa59cbf",
        "id": "2bTRKAL664pO"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((tensor([[[42, 40, 27,  ..., 44, 39, 34],\n",
              "           [42, 40, 26,  ..., 43, 39, 34],\n",
              "           [27, 29, 36,  ..., 26, 29, 32],\n",
              "           ...,\n",
              "           [42, 39, 28,  ..., 44, 38, 33],\n",
              "           [39, 38, 29,  ..., 39, 37, 33],\n",
              "           [31, 32, 35,  ..., 31, 32, 33]]], dtype=torch.uint8),\n",
              "  tensor([[[42, 40, 27,  ..., 44, 39, 34],\n",
              "           [42, 40, 26,  ..., 43, 39, 34],\n",
              "           [27, 29, 36,  ..., 26, 29, 32],\n",
              "           ...,\n",
              "           [42, 39, 28,  ..., 44, 38, 33],\n",
              "           [39, 38, 29,  ..., 39, 37, 33],\n",
              "           [31, 32, 35,  ..., 31, 32, 33]]], dtype=torch.uint8)),\n",
              " 6)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from torchvision.io import read_image\n",
        "from PIL import Image\n",
        "\n",
        "train_img_dir = '/content/12kDriveEnd_img0.2/train'\n",
        "val_img_dir = '/content/12kDriveEnd_img0.2/val'\n",
        "test_img_dir = '/content/12kDriveEnd_img0.2/test'\n",
        "class CustomImageDataset(Dataset):\n",
        "    # def __init__(self, file_name_list, img_dir,img_transform):\n",
        "    def __init__(self, file_name_list, img_dir):\n",
        "        self.file_name_list = file_name_list\n",
        "        self.img_dir = img_dir\n",
        "        # self.img_transform = img_transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.file_name_list)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        file_name = self.file_name_list[idx]\n",
        "        img_path_m = os.path.join(self.img_dir, self.file_name_list[idx]+'-M.png')\n",
        "        img_path_g = os.path.join(self.img_dir, self.file_name_list[idx]+'-G.png')\n",
        "        image_m = read_image(img_path_m)\n",
        "        image_g = read_image(img_path_m)\n",
        "        # image_m = Image.open(img_path_m)\n",
        "        # image_g = Image.open(img_path_g)\n",
        "        # image_tensor_m = self.img_transform(image_m)\n",
        "        # image_tensor_g = self.img_transform(image_g)\n",
        "        # label_name = file_name.split('_')[0]\n",
        "        # label_id = le.transform([label_name])[0]\n",
        "        label_id = get_label_num(file_name)\n",
        "\n",
        "        return (image_m,image_g), label_id\n",
        "\n",
        "# train_img_dataset = CustomImageDataset( file_name_list, train_img_dir,img_transform)\n",
        "# val_img_dataset = CustomImageDataset( file_name_list, val_img_dir,img_transform)\n",
        "# test_img_dataset = CustomImageDataset( file_name_list, test_img_dir,img_transform)\n",
        "train_img_dataset = CustomImageDataset( train_file_name_list, train_img_dir)\n",
        "val_img_dataset = CustomImageDataset( val_file_name_list, val_img_dir)\n",
        "test_img_dataset = CustomImageDataset( test_file_name_list, test_img_dir)\n",
        "train_img_dataset[2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "eD1JSOfM64pP"
      },
      "outputs": [],
      "source": [
        "train_dataloader = DataLoader(train_img_dataset, batch_size=64, shuffle=True)\n",
        "val_dataloader = DataLoader(val_img_dataset, batch_size=64, shuffle=True)\n",
        "test_dataloader = DataLoader(test_img_dataset, batch_size=64, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "sFqk_z-R_HOf"
      },
      "outputs": [],
      "source": [
        "\n",
        "import torch\n",
        "import torch.nn.utils.prune as prune\n",
        "import torchvision\n",
        "import random\n",
        "import sys\n",
        "import torch_pruning\n",
        "# sys.path.append(\"/content/Model-Compression\")\n",
        "\n",
        "# from model import *\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "random.seed(777)\n",
        "torch.manual_seed(777)\n",
        "if device == 'cuda':\n",
        "    torch.cuda.manual_seed(777)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = 1e-4\n",
        "batch_size = 512\n",
        "epoch = 20\n",
        "     "
      ],
      "metadata": {
        "id": "_Y-MyaJdnSc6"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "root = \"./CIFAR100\"\n",
        "transform = torchvision.transforms.ToTensor()\n",
        "data_train = torchvision.datasets.CIFAR100(root, train = True, transform = transform, download = True)\n",
        "data_test = torchvision.datasets.CIFAR100(root, train = False, transform = transform, download = True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101,
          "referenced_widgets": [
            "224c53c7cd4e45828f646cbd50010808",
            "7050dc771e8a434ca0bcd5bea422540a",
            "9fb954230742487694688ecb70d2fde4",
            "c2dd901524f445be86e615187eacc0be",
            "00ba53c0c0d64f9cbd543c87aa97351a",
            "853ac712ee22480689fd3d18df40904d",
            "29d6adf067f44a5d8d5e6855d58e43ed",
            "2564096686ef4017b379a83321f9b129",
            "0e831f6e7618438db3921f3b5868fc45",
            "40373bc809854fbea79abdce6c4329bd",
            "a096d8f4f4fd41cda675521c045a88f5"
          ]
        },
        "id": "Lrkl7PWTnSV_",
        "outputId": "8a268638-f886-4e99-8654-dc99fc439b5c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz to ./CIFAR100/cifar-100-python.tar.gz\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/169001437 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "224c53c7cd4e45828f646cbd50010808"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./CIFAR100/cifar-100-python.tar.gz to ./CIFAR100\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "train_loader = torch.utils.data.DataLoader(dataset=data_train, batch_size=batch_size, shuffle=True, drop_last=True)\n",
        "test_loader = torch.utils.data.DataLoader(dataset=data_test, batch_size=batch_size, shuffle=False, drop_last=True)"
      ],
      "metadata": {
        "id": "ZPrxGRqinSLW"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = final_model_2.to(device)\n",
        "checkpoint = torch.load(\"/content/Model-Compression/model.pth\")\n",
        "model.load_state_dict(checkpoint)\n",
        "criterion = torch.nn.CrossEntropyLoss().to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)"
      ],
      "metadata": {
        "id": "Dh-DEzrWBQh7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model = ResNet(Bottleneck, [3, 4, 6, 3], num_classes=100).to(device)\n",
        "# checkpoint = torch.load(\"/content/Model-Compression/model.pth\")\n",
        "# model.load_state_dict(checkpoint)\n",
        "# criterion = torch.nn.CrossEntropyLoss().to(device)\n",
        "# optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)"
      ],
      "metadata": {
        "id": "xqkY1AopnR6Q"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ItGF8HoYDARh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XzXRTbhsDMMQ"
      },
      "outputs": [],
      "source": [
        "def train_one_epoch(print_result = False):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    accuracy_sum = 0.0\n",
        "    length = 0\n",
        "\n",
        "    for i, data in enumerate(train_dataloader, 0):\n",
        "        # get the inputs; data is a list of [inputs, labels]\n",
        "        (img_m,img_g), labels = data\n",
        "\n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # forward + backward + optimize\n",
        "        outputs = final_model_1(img_m,img_g)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # print statistics\n",
        "        running_loss += loss.item()\n",
        "        if i % 10 == 9:    # print every 2000 mini-batches\n",
        "            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 10:.3f}')\n",
        "            running_loss = 0.0\n",
        "    if(print_result):\n",
        "        print(\"loss :\", loss_sum / length)\n",
        "        print(\"accuracy:\", accuracy_sum / length)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train_one_epoch(print_result = False):\n",
        "    model.train()\n",
        "    loss_sum = 0.0\n",
        "    accuracy_sum = 0.0\n",
        "    length = 0\n",
        "\n",
        "    for X, Y in train_loader:\n",
        "        X = X.to(device)\n",
        "        Y = Y.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        pred = model(X)\n",
        "        loss = criterion(pred, Y)\n",
        "        pred_idx = torch.argmax(pred, 1)\n",
        "        loss_sum += loss.item()\n",
        "        accuracy_sum += torch.sum((pred_idx == Y).float()).item()\n",
        "        length += X.size(0)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    if(print_result):\n",
        "        print(\"loss :\", loss_sum / length)\n",
        "        print(\"accuracy:\", accuracy_sum / length)"
      ],
      "metadata": {
        "id": "eNiHkCQpnYsT"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def eval():\n",
        "  with torch.no_grad():\n",
        "      model.eval()\n",
        "      running_loss = 0.0\n",
        "      accuracy_sum = 0.0\n",
        "      length = 0  \n",
        "      for i, data in enumerate(train_dataloader, 0):\n",
        "        # get the inputs; data is a list of [inputs, labels]\n",
        "        (img_m,img_g), labels = data\n",
        "\n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # forward + backward + optimize\n",
        "        outputs = final_model_1(img_m,img_g)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # print statistics\n",
        "        running_loss += loss.item()\n",
        "        if i % 10 == 9:    # print every 2000 mini-batches\n",
        "            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 10:.3f}')\n",
        "            running_loss = 0.0\n"
      ],
      "metadata": {
        "id": "vMcd3UybEoVu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def eval():\n",
        "#     with torch.no_grad():\n",
        "#         model.eval()\n",
        "#         loss_sum = 0.0\n",
        "#         accuracy_sum = 0.0\n",
        "#         length = 0\n",
        "\n",
        "#         for X, Y in train_loader:\n",
        "#             X = X.to(device)\n",
        "#             Y = Y.to(device)\n",
        "\n",
        "#             pred = model(X)\n",
        "#             loss = criterion(pred, Y)\n",
        "#             pred_idx = torch.argmax(pred, 1)\n",
        "#             loss_sum += loss.item()\n",
        "#             accuracy_sum += torch.sum((pred_idx == Y).float()).item()\n",
        "#             length += X.size(0)\n",
        "            \n",
        "#         print(\"loss :\", loss_sum / length)\n",
        "#         print(\"accuracy:\", accuracy_sum / length)"
      ],
      "metadata": {
        "id": "jpYBJX3invDb"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train():\n",
        "    for i in range(epoch):\n",
        "        print(\"EPOCH[\" + str(i + 1) + \"]\")\n",
        "        print(\"==== train ====\")\n",
        "        train_one_epoch(print_result=True)\n",
        "        \n",
        "        print(\"==== eval ====\")\n",
        "        eval()"
      ],
      "metadata": {
        "id": "0-lVYlUqG83F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def train():\n",
        "#     for i in range(epoch):\n",
        "#         print(\"EPOCH[\" + str(i + 1) + \"]\")\n",
        "#         print(\"==== train ====\")\n",
        "#         train_one_epoch(print_result=True)\n",
        "        \n",
        "#         print(\"==== eval ====\")\n",
        "#         eval()\n",
        "     "
      ],
      "metadata": {
        "id": "_a7Nl5Mqnu89"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# code for pruning whole model\n",
        "\n",
        "example_inputs = torch.randn(64, 1, 200, 200).to(\"cuda\")\n",
        "imp = torch_pruning.importance.MagnitudeImportance(p = 2)\n",
        "ignored_layers = []\n",
        "for m in model.modules():\n",
        "    if isinstance(m, torch.nn.Linear) and m.out_features == 100:\n",
        "        ignored_layers.append(m)\n",
        "\n",
        "#ch_sparsity = 0.5 means 50% of the filters are pruned\n",
        "pruner = torch_pruning.pruner.MagnitudePruner(model=model, example_inputs=example_inputs, importance=imp, iterative_steps=5, ch_sparsity=0.7, ignored_layers=ignored_layers)\n",
        "\n",
        "base_macs, base_nparams = torch_pruning.utils.count_ops_and_params(model=model, example_inputs=example_inputs)\n",
        "print(\"basemacs :\", base_macs, \"basenparams :\", base_nparams)\n",
        "for i in range(5):\n",
        "    pruner.step()\n",
        "    macs, nparams = torch_pruning.utils.count_ops_and_params(model=model, example_inputs=example_inputs)\n",
        "    print(\"iter [\", i, \"] macs :\", macs, \"nparams :\", nparams)\n",
        "\n",
        "    #finetune\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)\n",
        "\n",
        "    print(\"==== train ====\")\n",
        "    for j in range(10):\n",
        "        train_one_epoch(print_result = True)\n",
        "    \n",
        "    print(\"==== eval ====\")\n",
        "    eval()"
      ],
      "metadata": {
        "id": "DkkfyY6KHMLF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # code for pruning whole model\n",
        "\n",
        "# example_inputs = torch.randn(1, 3, 32, 32).to(\"cuda\")\n",
        "# imp = torch_pruning.importance.MagnitudeImportance(p = 2)\n",
        "# ignored_layers = []\n",
        "# for m in model.modules():\n",
        "#     if isinstance(m, torch.nn.Linear) and m.out_features == 100:\n",
        "#         ignored_layers.append(m)\n",
        "\n",
        "# #ch_sparsity = 0.5 means 50% of the filters are pruned\n",
        "# pruner = torch_pruning.pruner.MagnitudePruner(model=model, example_inputs=example_inputs, importance=imp, iterative_steps=5, ch_sparsity=0.7, ignored_layers=ignored_layers)\n",
        "\n",
        "# base_macs, base_nparams = torch_pruning.utils.count_ops_and_params(model=model, example_inputs=example_inputs)\n",
        "# print(\"basemacs :\", base_macs, \"basenparams :\", base_nparams)\n",
        "# for i in range(5):\n",
        "#     pruner.step()\n",
        "#     macs, nparams = torch_pruning.utils.count_ops_and_params(model=model, example_inputs=example_inputs)\n",
        "#     print(\"iter [\", i, \"] macs :\", macs, \"nparams :\", nparams)\n",
        "\n",
        "#     #finetune\n",
        "#     optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)\n",
        "\n",
        "#     print(\"==== train ====\")\n",
        "#     for j in range(10):\n",
        "#         train_one_epoch(print_result = True)\n",
        "    \n",
        "#     print(\"==== eval ====\")\n",
        "#     eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "caTU0R8Bnu56",
        "outputId": "bebaf23f-27b7-4bb1-c4cb-a94a9d0bfb9e"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "basemacs : 84284004.0 basenparams : 23712932\n",
            "iter [ 0 ] macs : 62645652.0 nparams : 17548123\n",
            "==== train ====\n",
            "loss : 0.000151031017069436\n",
            "accuracy: 0.975354381443299\n",
            "loss : 9.503110687574852e-05\n",
            "accuracy: 0.9847575708762887\n",
            "loss : 6.685144127032734e-05\n",
            "accuracy: 0.9900531572164949\n",
            "loss : 5.007407867431295e-05\n",
            "accuracy: 0.9928519652061856\n",
            "loss : 4.1293786313131536e-05\n",
            "accuracy: 0.9951272551546392\n",
            "loss : 3.213056150287361e-05\n",
            "accuracy: 0.9965769974226805\n",
            "loss : 2.7985589815553915e-05\n",
            "accuracy: 0.9974629510309279\n",
            "loss : 2.3689339952360353e-05\n",
            "accuracy: 0.9982482280927835\n",
            "loss : 2.129067128332759e-05\n",
            "accuracy: 0.9985502577319587\n",
            "loss : 1.859226203939131e-05\n",
            "accuracy: 0.9987717461340206\n",
            "==== eval ====\n",
            "loss : 1.0184744374764142e-05\n",
            "accuracy: 0.999597293814433\n",
            "iter [ 1 ] macs : 44222598.0 nparams : 12313842\n",
            "==== train ====\n",
            "loss : 0.0013549754914549208\n",
            "accuracy: 0.827722293814433\n",
            "loss : 0.0010884487515401824\n",
            "accuracy: 0.8513611469072165\n",
            "loss : 0.0009099636448921694\n",
            "accuracy: 0.870227931701031\n",
            "loss : 0.0007948138650276305\n",
            "accuracy: 0.881946681701031\n",
            "loss : 0.0006860284213956023\n",
            "accuracy: 0.8942292203608248\n",
            "loss : 0.000604210925209288\n",
            "accuracy: 0.9048002577319587\n",
            "loss : 0.0005371441714196783\n",
            "accuracy: 0.9135591172680413\n",
            "loss : 0.0004772673611236333\n",
            "accuracy: 0.9226401417525774\n",
            "loss : 0.0004325536602973631\n",
            "accuracy: 0.9294257409793815\n",
            "loss : 0.0003826376524773071\n",
            "accuracy: 0.9365335051546392\n",
            "==== eval ====\n",
            "loss : 0.0002981214637066401\n",
            "accuracy: 0.953125\n",
            "iter [ 2 ] macs : 28987207.0 nparams : 8000766\n",
            "==== train ====\n",
            "loss : 0.00549942825815266\n",
            "accuracy: 0.5423848260309279\n",
            "loss : 0.004723627643525293\n",
            "accuracy: 0.5756080863402062\n",
            "loss : 0.00420291328409053\n",
            "accuracy: 0.6004953286082474\n",
            "loss : 0.0038261335371926274\n",
            "accuracy: 0.6186171069587629\n",
            "loss : 0.003523293400144915\n",
            "accuracy: 0.6352287371134021\n",
            "loss : 0.003253758721744891\n",
            "accuracy: 0.6483972293814433\n",
            "loss : 0.0030495070626709573\n",
            "accuracy: 0.6603978737113402\n",
            "loss : 0.0028616874214722633\n",
            "accuracy: 0.6736871778350515\n",
            "loss : 0.002699918653719972\n",
            "accuracy: 0.6830098260309279\n",
            "loss : 0.0025450042568953687\n",
            "accuracy: 0.6931781572164949\n",
            "==== eval ====\n",
            "loss : 0.002345697435828828\n",
            "accuracy: 0.7099105992268041\n",
            "iter [ 3 ] macs : 16994237.0 nparams : 4636408\n",
            "==== train ====\n",
            "loss : 0.011179941879182132\n",
            "accuracy: 0.28980750644329895\n",
            "loss : 0.0100583894677537\n",
            "accuracy: 0.31568137886597936\n",
            "loss : 0.009243151182596832\n",
            "accuracy: 0.3352931701030928\n",
            "loss : 0.008604076907000284\n",
            "accuracy: 0.34914626288659795\n",
            "loss : 0.008055129779752382\n",
            "accuracy: 0.36336179123711343\n",
            "loss : 0.007601102727666958\n",
            "accuracy: 0.3759262242268041\n",
            "loss : 0.007211910762353656\n",
            "accuracy: 0.3869201030927835\n",
            "loss : 0.00686142673312696\n",
            "accuracy: 0.39352448453608246\n",
            "loss : 0.006575619849885248\n",
            "accuracy: 0.4033706507731959\n",
            "loss : 0.00628718093863314\n",
            "accuracy: 0.4115858569587629\n",
            "==== eval ====\n",
            "loss : 0.006030790987856609\n",
            "accuracy: 0.42249919458762886\n",
            "iter [ 4 ] macs : 8156922.0 nparams : 2170522\n",
            "==== train ====\n",
            "loss : 0.01609811876161197\n",
            "accuracy: 0.11374436211340207\n",
            "loss : 0.014701732175098252\n",
            "accuracy: 0.12868476159793815\n",
            "loss : 0.013582498353627539\n",
            "accuracy: 0.14159149484536082\n",
            "loss : 0.012662747087552375\n",
            "accuracy: 0.15121617268041238\n",
            "loss : 0.01190561791593881\n",
            "accuracy: 0.15866623711340205\n",
            "loss : 0.011234413736413434\n",
            "accuracy: 0.1657740012886598\n",
            "loss : 0.01067522596375844\n",
            "accuracy: 0.17475434922680413\n",
            "loss : 0.010200673445444746\n",
            "accuracy: 0.18176143685567012\n",
            "loss : 0.009758640781582631\n",
            "accuracy: 0.18572809278350516\n",
            "loss : 0.009359200081797605\n",
            "accuracy: 0.19112435567010308\n",
            "==== eval ====\n",
            "loss : 0.00917024068419159\n",
            "accuracy: 0.19422519329896906\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DduhTqj2nu2v",
        "outputId": "fe21ae1d-cbda-4d18-9798-ca89dfe06864"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss : 0.009169269487569012\n",
            "accuracy: 0.19418492268041238\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "75c3UWRQnuzz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SdhkpW2lnuwi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sO2CpxT5nupJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "crRN3B6DnYk5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kWC6pf50_BUN"
      },
      "source": [
        "#Faulting signals transformed into imgs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e8kdA1YuyCG6"
      },
      "source": [
        "##GAF preprocessing with Equal difference 0.2\n",
        "改成后缀-G,并和-M放到一个文件夹"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1PDKmiu1J0Xh",
        "outputId": "6756bd17-e231-471d-8a48-5102d145d188"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyts\n",
            "  Downloading pyts-0.12.0-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m32.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scikit-learn>=0.22.1 in /usr/local/lib/python3.8/dist-packages (from pyts) (1.2.1)\n",
            "Requirement already satisfied: joblib>=0.12 in /usr/local/lib/python3.8/dist-packages (from pyts) (1.2.0)\n",
            "Requirement already satisfied: numba>=0.48.0 in /usr/local/lib/python3.8/dist-packages (from pyts) (0.56.4)\n",
            "Requirement already satisfied: numpy>=1.17.5 in /usr/local/lib/python3.8/dist-packages (from pyts) (1.22.4)\n",
            "Requirement already satisfied: scipy>=1.3.0 in /usr/local/lib/python3.8/dist-packages (from pyts) (1.10.1)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.8/dist-packages (from numba>=0.48.0->pyts) (6.0.0)\n",
            "Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /usr/local/lib/python3.8/dist-packages (from numba>=0.48.0->pyts) (0.39.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from numba>=0.48.0->pyts) (57.4.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn>=0.22.1->pyts) (3.1.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata->numba>=0.48.0->pyts) (3.15.0)\n",
            "Installing collected packages: pyts\n",
            "Successfully installed pyts-0.12.0\n"
          ]
        }
      ],
      "source": [
        "!pip install pyts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XM9vKGT3KAH-",
        "outputId": "c02ddca7-ff4a-4483-8bb9-d7e99c096874"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Motor_Bearing_Fault_Diagnosis'...\n",
            "remote: Enumerating objects: 116, done.\u001b[K\n",
            "remote: Total 116 (delta 0), reused 0 (delta 0), pack-reused 116\u001b[K\n",
            "Receiving objects: 100% (116/116), 65.34 MiB | 18.80 MiB/s, done.\n",
            "Resolving deltas: 100% (16/16), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/zzwalala/Motor_Bearing_Fault_Diagnosis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lqex9O-yyCG9",
        "outputId": "120ee4df-4faf-43f9-cbf8-d1bf6f0ae598"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " 51\n",
            "\n",
            "X147_DE_time\n",
            "122281\n",
            "OR007@3_3 train\n",
            "Total number of train_images numbers : 769\n",
            "train images number in folder:0\n",
            "Finished creating img files for OR007@3_3.mat\n",
            "\n",
            "X159_DE_time\n",
            "122281\n",
            "OR007@12_2 test\n",
            "Total number of test_images numbers : 192\n",
            "Finished creating img files for OR007@12_2.mat\n",
            "\n",
            "X158_DE_time\n",
            "121991\n",
            "OR007@12_1 val\n",
            "Total number of val_images numbers : 192\n",
            "Finished creating img files for OR007@12_1.mat\n",
            "\n",
            "X211_DE_time\n",
            "121846\n",
            "IR021_2 test\n",
            "Total number of test_images numbers : 192\n",
            "Finished creating img files for IR021_2.mat\n",
            "\n",
            "X197_DE_time\n",
            "121846\n",
            "OR014@6_0 train\n",
            "Total number of train_images numbers : 769\n",
            "train images number in folder:0\n",
            "Finished creating img files for OR014@6_0.mat\n",
            "\n",
            "X169_DE_time\n",
            "121846\n",
            "IR014_0 train\n",
            "Total number of train_images numbers : 769\n",
            "train images number in folder:0\n",
            "Finished creating img files for IR014_0.mat\n",
            "\n",
            "X130_DE_time\n",
            "121991\n",
            "OR007@6_0 train\n",
            "Total number of train_images numbers : 769\n",
            "train images number in folder:0\n",
            "Finished creating img files for OR007@6_0.mat\n",
            "\n",
            "X234_DE_time\n",
            "122426\n",
            "OR021@6_0 train\n",
            "Total number of train_images numbers : 769\n",
            "train images number in folder:0\n",
            "Finished creating img files for OR021@6_0.mat\n",
            "\n",
            "X146_DE_time\n",
            "121556\n",
            "OR007@3_2 test\n",
            "Total number of test_images numbers : 192\n",
            "Finished creating img files for OR007@3_2.mat\n",
            "\n",
            "X188_DE_time\n",
            "122136\n",
            "B014_3 train\n",
            "Total number of train_images numbers : 769\n",
            "train images number in folder:0\n",
            "Finished creating img files for B014_3.mat\n",
            "\n",
            "X223_DE_time\n",
            "121701\n",
            "B021_1 val\n",
            "Total number of val_images numbers : 192\n",
            "Finished creating img files for B021_1.mat\n",
            "\n",
            "X133_DE_time\n",
            "122571\n",
            "OR007@6_3 train\n",
            "Total number of train_images numbers : 769\n",
            "train images number in folder:0\n",
            "Finished creating img files for OR007@6_3.mat\n",
            "\n",
            "X198_DE_time\n",
            "122136\n",
            "OR014@6_1 val\n",
            "Total number of val_images numbers : 192\n",
            "Finished creating img files for OR014@6_1.mat\n",
            "\n",
            "X237_DE_time\n",
            "121991\n",
            "OR021@6_3 train\n",
            "Total number of train_images numbers : 769\n",
            "train images number in folder:0\n",
            "Finished creating img files for OR021@6_3.mat\n",
            "\n",
            "X156_DE_time\n",
            "122281\n",
            "OR007@12_0 train\n",
            "Total number of train_images numbers : 769\n",
            "train images number in folder:0\n",
            "Finished creating img files for OR007@12_0.mat\n",
            "\n",
            "X145_DE_time\n",
            "121846\n",
            "OR007@3_1 val\n",
            "Total number of val_images numbers : 192\n",
            "Finished creating img files for OR007@3_1.mat\n",
            "\n",
            "X107_DE_time\n",
            "122136\n",
            "IR007_2 test\n",
            "Total number of test_images numbers : 192\n",
            "Finished creating img files for IR007_2.mat\n",
            "\n",
            "X224_DE_time\n",
            "122136\n",
            "B021_2 test\n",
            "Total number of test_images numbers : 192\n",
            "Finished creating img files for B021_2.mat\n",
            "\n",
            "X222_DE_time\n",
            "121991\n",
            "B021_0 train\n",
            "Total number of train_images numbers : 769\n",
            "train images number in folder:0\n",
            "Finished creating img files for B021_0.mat\n",
            "\n",
            "X259_DE_time\n",
            "122426\n",
            "OR021@12_1 val\n",
            "Total number of val_images numbers : 192\n",
            "Finished creating img files for OR021@12_1.mat\n",
            "\n",
            "X121_DE_time\n",
            "121556\n",
            "B007_3 train\n",
            "Total number of train_images numbers : 769\n",
            "train images number in folder:0\n",
            "Finished creating img files for B007_3.mat\n",
            "\n",
            "X199_DE_time\n",
            "121846\n",
            "OR014@6_2 test\n",
            "Total number of test_images numbers : 192\n",
            "Finished creating img files for OR014@6_2.mat\n",
            "\n",
            "X171_DE_time\n",
            "121846\n",
            "IR014_2 test\n",
            "Total number of test_images numbers : 192\n",
            "Finished creating img files for IR014_2.mat\n",
            "\n",
            "X185_DE_time\n",
            "121846\n",
            "B014_0 train\n",
            "Total number of train_images numbers : 769\n",
            "train images number in folder:0\n",
            "Finished creating img files for B014_0.mat\n",
            "\n",
            "X120_DE_time\n",
            "121556\n",
            "B007_2 test\n",
            "Total number of test_images numbers : 192\n",
            "Finished creating img files for B007_2.mat\n",
            "\n",
            "X236_DE_time\n",
            "122281\n",
            "OR021@6_2 test\n",
            "Total number of test_images numbers : 192\n",
            "Finished creating img files for OR021@6_2.mat\n",
            "\n",
            "X212_DE_time\n",
            "121991\n",
            "IR021_3 train\n",
            "Total number of train_images numbers : 769\n",
            "train images number in folder:0\n",
            "Finished creating img files for IR021_3.mat\n",
            "\n",
            "X247_DE_time\n",
            "121991\n",
            "OR021@3_1 val\n",
            "Total number of val_images numbers : 192\n",
            "Finished creating img files for OR021@3_1.mat\n",
            "\n",
            "X210_DE_time\n",
            "121556\n",
            "IR021_1 val\n",
            "Total number of val_images numbers : 192\n",
            "Finished creating img files for IR021_1.mat\n",
            "\n",
            "X248_DE_time\n",
            "122281\n",
            "OR021@3_2 test\n",
            "Total number of test_images numbers : 192\n",
            "Finished creating img files for OR021@3_2.mat\n",
            "\n",
            "X108_DE_time\n",
            "122917\n",
            "IR007_3 train\n",
            "Total number of train_images numbers : 769\n",
            "train images number in folder:0\n",
            "Finished creating img files for IR007_3.mat\n",
            "\n",
            "X260_DE_time\n",
            "122716\n",
            "OR021@12_2 test\n",
            "Total number of test_images numbers : 192\n",
            "Finished creating img files for OR021@12_2.mat\n",
            "\n",
            "X186_DE_time\n",
            "122136\n",
            "B014_1 val\n",
            "Total number of val_images numbers : 192\n",
            "Finished creating img files for B014_1.mat\n",
            "\n",
            "X225_DE_time\n",
            "122136\n",
            "B021_3 train\n",
            "Total number of train_images numbers : 769\n",
            "train images number in folder:0\n",
            "Finished creating img files for B021_3.mat\n",
            "\n",
            "X106_DE_time\n",
            "121991\n",
            "IR007_1 val\n",
            "Total number of val_images numbers : 192\n",
            "Finished creating img files for IR007_1.mat\n",
            "\n",
            "X105_DE_time\n",
            "121265\n",
            "IR007_0 train\n",
            "Total number of train_images numbers : 769\n",
            "train images number in folder:0\n",
            "Finished creating img files for IR007_0.mat\n",
            "\n",
            "X160_DE_time\n",
            "122136\n",
            "OR007@12_3 train\n",
            "Total number of train_images numbers : 769\n",
            "train images number in folder:0\n",
            "Finished creating img files for OR007@12_3.mat\n",
            "\n",
            "X246_DE_time\n",
            "121701\n",
            "OR021@3_0 train\n",
            "Total number of train_images numbers : 769\n",
            "train images number in folder:0\n",
            "Finished creating img files for OR021@3_0.mat\n",
            "\n",
            "X249_DE_time\n",
            "122136\n",
            "OR021@3_3 train\n",
            "Total number of train_images numbers : 769\n",
            "train images number in folder:0\n",
            "Finished creating img files for OR021@3_3.mat\n",
            "\n",
            "X209_DE_time\n",
            "122136\n",
            "IR021_0 train\n",
            "Total number of train_images numbers : 769\n",
            "train images number in folder:0\n",
            "Finished creating img files for IR021_0.mat\n",
            "\n",
            "X261_DE_time\n",
            "121701\n",
            "OR021@12_3 train\n",
            "Total number of train_images numbers : 769\n",
            "train images number in folder:0\n",
            "Finished creating img files for OR021@12_3.mat\n",
            "\n",
            "X131_DE_time\n",
            "122426\n",
            "OR007@6_1 val\n",
            "Total number of val_images numbers : 192\n",
            "Finished creating img files for OR007@6_1.mat\n",
            "\n",
            "X258_DE_time\n",
            "121846\n",
            "OR021@12_0 train\n",
            "Total number of train_images numbers : 769\n",
            "train images number in folder:0\n",
            "Finished creating img files for OR021@12_0.mat\n",
            "\n",
            "X200_DE_time\n",
            "121991\n",
            "OR014@6_3 train\n",
            "Total number of train_images numbers : 769\n",
            "train images number in folder:0\n",
            "Finished creating img files for OR014@6_3.mat\n",
            "\n",
            "X235_DE_time\n",
            "121991\n",
            "OR021@6_1 val\n",
            "Total number of val_images numbers : 192\n",
            "Finished creating img files for OR021@6_1.mat\n",
            "\n",
            "X170_DE_time\n",
            "121846\n",
            "IR014_1 val\n",
            "Total number of val_images numbers : 192\n",
            "Finished creating img files for IR014_1.mat\n",
            "\n",
            "X172_DE_time\n",
            "121701\n",
            "IR014_3 train\n",
            "Total number of train_images numbers : 769\n",
            "train images number in folder:0\n",
            "Finished creating img files for IR014_3.mat\n",
            "\n",
            "X119_DE_time\n",
            "121410\n",
            "B007_1 val\n",
            "Total number of val_images numbers : 192\n",
            "Finished creating img files for B007_1.mat\n",
            "\n",
            "X118_DE_time\n",
            "122571\n",
            "B007_0 train\n",
            "Total number of train_images numbers : 769\n",
            "train images number in folder:0\n",
            "Finished creating img files for B007_0.mat\n",
            "\n",
            "X187_DE_time\n",
            "121991\n",
            "B014_2 test\n",
            "Total number of test_images numbers : 192\n",
            "Finished creating img files for B014_2.mat\n",
            "\n",
            "X132_DE_time\n",
            "121410\n",
            "OR007@6_2 test\n",
            "Total number of test_images numbers : 192\n",
            "Finished creating img files for OR007@6_2.mat\n",
            "- - - - - - - - - - - - - - - - - - - -\n",
            "19225\n"
          ]
        }
      ],
      "source": [
        "import scipy.io\n",
        "import random\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import cv2\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import image\n",
        "from pyts.image import GramianAngularField\n",
        "import time\n",
        "import math\n",
        "# def folder2img(save_path, folder_path, train_num, val_num, test_num, img_size):\n",
        "def folder2img(save_path, folder_path, train_num, val_num, test_num):\n",
        "    files = os.listdir(folder_path)\n",
        "    print(f'\\n {len(files)}')\n",
        "    if not os.path.exists(save_path + '/train'):\n",
        "        os.makedirs(save_path + '/train')\n",
        "    if not os.path.exists(save_path + '/val'):\n",
        "        os.makedirs(save_path + '/val')\n",
        "    if not os.path.exists(save_path + '/test'):\n",
        "        os.makedirs(save_path + '/test')\n",
        "    for file in files:\n",
        "        \n",
        "     if os.path.splitext(file)[1] == '.mat':\n",
        "          file_path = folder_path + \"/\" + file\n",
        "          raw_data = scipy.io.loadmat(file_path)\n",
        "          for name in raw_data.keys():\n",
        "              if \"DE\" in name:\n",
        "                  column_DE = name\n",
        "                  print(f'\\n{column_DE}')\n",
        "          data_DE = raw_data[column_DE]\n",
        "          \n",
        "          n_sample,_, = data_DE.shape\n",
        "          print(n_sample)\n",
        "          final_data=data_DE\n",
        "            \n",
        "          # im_size = 128\n",
        "\n",
        "          series = 1000   # each time series\n",
        "\n",
        "          # im_size = 200\n",
        "          # step = 1000  # step of slide window\n",
        "          \n",
        "          ss = n_sample - series\n",
        "          # im_sum = (n_sample - im_size) // step\n",
        "          gasf = GramianAngularField(image_size=0.2, method='difference')\n",
        "          if (\"_3\" in os.path.splitext(file)[0]) or (\"_0\" in os.path.splitext(file)[0]):\n",
        "                # spacing_len = （signal_len - im_size） //  train_num   报错无效字符，不能用小括号吗\n",
        "                spacing_len = ss //  train_num\n",
        "                choice = list(range(0, train_num*spacing_len, spacing_len))\n",
        "                # choices = [i for i in range(data_DE.shape[0] - im_size)]  \n",
        "                print(os.path.splitext(file)[0], \"train\")              \n",
        "                # choice = random.sample(choices ,train_num)\n",
        "                \n",
        "                count=0\n",
        "                for i in choice :\n",
        "                # for i in range(im_sum):\n",
        "                  start_number = i\n",
        "                  # start_index, end_index = i * step, i * step + im_size\n",
        "                  start_index, end_index = i , i + series\n",
        "                  # print(start_index, end_index)\n",
        "                  \n",
        "                  sub_series = final_data[start_index:end_index,:].reshape(1,-1)\n",
        "              \n",
        "                  # print(sub_series)\n",
        "                  # print(sub_series.shape)  #(1,128)\n",
        "                  GAF_gasf = gasf.fit_transform(sub_series)\n",
        "                  im = GAF_gasf[0]\n",
        "\n",
        "                  im=im*255\n",
        "                  \n",
        "                  # filename = save_path + '%d.png' % i\n",
        "                  # image.imsave(filename, im)\n",
        "                  cv2.imwrite(save_path + '/train/' + os.path.splitext(file)[0] + '_' + str(i) + '-G' + '.png', im)\n",
        "                  count+=1\n",
        "                print(f'Total number of train_images numbers : {count}')  # 输出总数\n",
        "                path='/content/12kDriveEnd_img/train'\n",
        "                counter = 0\n",
        "                for root,dirs,files in os.walk(path):    #遍历统计\n",
        "                   for each in files:\n",
        "                     counter += 1   #统计文件夹下文件个数\n",
        "                print(f'train images number in folder:{counter}')               #输出结果\n",
        "          elif (\"_1\") in os.path.splitext(file)[0]:\n",
        "                spacing_len = ss //  val_num \n",
        "                choice = list(range(0, val_num*spacing_len, spacing_len))\n",
        "                # choices = [i for i in range(data_DE.shape[0] - im_size)]\n",
        "                print(os.path.splitext(file)[0], \"val\")\n",
        "                # choice = random.sample(choices, val_num)\n",
        "                count=0\n",
        "                for i in choice:\n",
        "                  start_number = i\n",
        "                  start_index, end_index = i , i + series\n",
        "                  \n",
        "                  \n",
        "                  sub_series = final_data[start_index:end_index,:].reshape(1,-1)\n",
        "                  GAF_gasf = gasf.fit_transform(sub_series)\n",
        "                  im = GAF_gasf[0]\n",
        "                  im=im*255\n",
        "                  cv2.imwrite(save_path + '/val/' + os.path.splitext(file)[0] + '_' + str(i) + '-G' + '.png', im)\n",
        "                  count+=1\n",
        "                print(f'Total number of val_images numbers : {count}')  # 输出总数\n",
        "                \n",
        "          elif (\"_2\") in os.path.splitext(file)[0]:\n",
        "                spacing_len = ss //  test_num \n",
        "                choice = list(range(0, test_num*spacing_len, spacing_len))\n",
        "                # choices = [i for i in range(data_DE.shape[0] - im_size)]\n",
        "                print(os.path.splitext(file)[0], \"test\")\n",
        "                # choice = random.sample(choices, test_num)\n",
        "                count=0\n",
        "                for i in choice:\n",
        "                  start_index, end_index = i , i + series\n",
        "                  \n",
        "                  # sub_series = final_data[:, start_index:end_index]\n",
        "                  sub_series = final_data[start_index:end_index,:].reshape(1,-1)\n",
        "                  # print(sub_series.shape)\n",
        "                  GAF_gasf = gasf.fit_transform(sub_series)\n",
        "                  im = GAF_gasf[0]\n",
        "                  im=im*255\n",
        "                  cv2.imwrite(save_path + '/test/' + os.path.splitext(file)[0] + '_' + str(i) + '-G' + '.png', im)\n",
        "                  count+=1\n",
        "                print(f'Total number of test_images numbers : {count}')  # 输出总数  \n",
        "          else:\n",
        "                print(\"wrong!\", os.path.splitext(file)[0])\n",
        "\n",
        "          print('Finished creating img files for ' + file)\n",
        "    print('- - - - - - - - - - - - - - - - - - - -')\n",
        "data_path = '/content/Motor_Bearing_Fault_Diagnosis/data/12kDriveEnd'\n",
        "normal_path = '/content/Motor_Bearing_Fault_Diagnosis/data/Normal_Baseline_Data'\n",
        "img_save_path = '/content/12kDriveEnd_img0.2'\n",
        "train_number = round(20000/26)  # Real train number/26\n",
        "val_number = round(2500/13)   # Real val number/13\n",
        "test_number = round(2500/13)  # Real test number/13\n",
        "# matrix_size =128 * 128\n",
        "\n",
        "# folder2img(img_save_path, data_path, train_number, val_number, test_number, matrix_size)\n",
        "folder2img(img_save_path, data_path, train_number, val_number, test_number)\n",
        "import os\n",
        "\n",
        "path='/content/12kDriveEnd_img0.2/train'\n",
        "count = 0\n",
        "for root,dirs,files in os.walk(path):    #遍历统计\n",
        "      for each in files:\n",
        "             count += 1   #统计文件夹下文件个数\n",
        "print(count)              #输出结果"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fu4kVWzqh9gO"
      },
      "source": [
        "##MAR preprocessing with Equal difference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jsgoTuc-iAx9",
        "outputId": "ed4cf566-c211-4a8a-e7b3-ede0c3e36652"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "51\n",
            "\n",
            "X147_DE_time\n",
            "122281\n",
            "OR007@3_3 train\n",
            "Total number of train_images  : 769\n",
            "train images number in folder:19994\n",
            "Finished creating img files for OR007@3_3.mat\n",
            "\n",
            "X159_DE_time\n",
            "122281\n",
            "OR007@12_2 test\n",
            "Total number of test_images  : 192\n",
            "Finished creating img files for OR007@12_2.mat\n",
            "\n",
            "X158_DE_time\n",
            "121991\n",
            "OR007@12_1 val\n",
            "Total number of val_images  : 192\n",
            "Finished creating img files for OR007@12_1.mat\n",
            "\n",
            "X211_DE_time\n",
            "121846\n",
            "IR021_2 test\n",
            "Total number of test_images  : 192\n",
            "Finished creating img files for IR021_2.mat\n",
            "\n",
            "X197_DE_time\n",
            "121846\n",
            "OR014@6_0 train\n",
            "Total number of train_images  : 769\n",
            "train images number in folder:20763\n",
            "Finished creating img files for OR014@6_0.mat\n",
            "\n",
            "X169_DE_time\n",
            "121846\n",
            "IR014_0 train\n",
            "Total number of train_images  : 769\n",
            "train images number in folder:21532\n",
            "Finished creating img files for IR014_0.mat\n",
            "\n",
            "X130_DE_time\n",
            "121991\n",
            "OR007@6_0 train\n",
            "Total number of train_images  : 769\n",
            "train images number in folder:22301\n",
            "Finished creating img files for OR007@6_0.mat\n",
            "\n",
            "X234_DE_time\n",
            "122426\n",
            "OR021@6_0 train\n",
            "Total number of train_images  : 769\n",
            "train images number in folder:23070\n",
            "Finished creating img files for OR021@6_0.mat\n",
            "\n",
            "X146_DE_time\n",
            "121556\n",
            "OR007@3_2 test\n",
            "Total number of test_images  : 192\n",
            "Finished creating img files for OR007@3_2.mat\n",
            "\n",
            "X188_DE_time\n",
            "122136\n",
            "B014_3 train\n",
            "Total number of train_images  : 769\n",
            "train images number in folder:23839\n",
            "Finished creating img files for B014_3.mat\n",
            "\n",
            "X223_DE_time\n",
            "121701\n",
            "B021_1 val\n",
            "Total number of val_images  : 192\n",
            "Finished creating img files for B021_1.mat\n",
            "\n",
            "X133_DE_time\n",
            "122571\n",
            "OR007@6_3 train\n",
            "Total number of train_images  : 769\n",
            "train images number in folder:24608\n",
            "Finished creating img files for OR007@6_3.mat\n",
            "\n",
            "X198_DE_time\n",
            "122136\n",
            "OR014@6_1 val\n",
            "Total number of val_images  : 192\n",
            "Finished creating img files for OR014@6_1.mat\n",
            "\n",
            "X237_DE_time\n",
            "121991\n",
            "OR021@6_3 train\n",
            "Total number of train_images  : 769\n",
            "train images number in folder:25377\n",
            "Finished creating img files for OR021@6_3.mat\n",
            "\n",
            "X156_DE_time\n",
            "122281\n",
            "OR007@12_0 train\n",
            "Total number of train_images  : 769\n",
            "train images number in folder:26146\n",
            "Finished creating img files for OR007@12_0.mat\n",
            "\n",
            "X145_DE_time\n",
            "121846\n",
            "OR007@3_1 val\n",
            "Total number of val_images  : 192\n",
            "Finished creating img files for OR007@3_1.mat\n",
            "\n",
            "X107_DE_time\n",
            "122136\n",
            "IR007_2 test\n",
            "Total number of test_images  : 192\n",
            "Finished creating img files for IR007_2.mat\n",
            "\n",
            "X224_DE_time\n",
            "122136\n",
            "B021_2 test\n",
            "Total number of test_images  : 192\n",
            "Finished creating img files for B021_2.mat\n",
            "\n",
            "X222_DE_time\n",
            "121991\n",
            "B021_0 train\n",
            "Total number of train_images  : 769\n",
            "train images number in folder:26915\n",
            "Finished creating img files for B021_0.mat\n",
            "\n",
            "X259_DE_time\n",
            "122426\n",
            "OR021@12_1 val\n",
            "Total number of val_images  : 192\n",
            "Finished creating img files for OR021@12_1.mat\n",
            "\n",
            "X121_DE_time\n",
            "121556\n",
            "B007_3 train\n",
            "Total number of train_images  : 769\n",
            "train images number in folder:27684\n",
            "Finished creating img files for B007_3.mat\n",
            "\n",
            "X199_DE_time\n",
            "121846\n",
            "OR014@6_2 test\n",
            "Total number of test_images  : 192\n",
            "Finished creating img files for OR014@6_2.mat\n",
            "\n",
            "X171_DE_time\n",
            "121846\n",
            "IR014_2 test\n",
            "Total number of test_images  : 192\n",
            "Finished creating img files for IR014_2.mat\n",
            "\n",
            "X185_DE_time\n",
            "121846\n",
            "B014_0 train\n",
            "Total number of train_images  : 769\n",
            "train images number in folder:28453\n",
            "Finished creating img files for B014_0.mat\n",
            "\n",
            "X120_DE_time\n",
            "121556\n",
            "B007_2 test\n",
            "Total number of test_images  : 192\n",
            "Finished creating img files for B007_2.mat\n",
            "\n",
            "X236_DE_time\n",
            "122281\n",
            "OR021@6_2 test\n",
            "Total number of test_images  : 192\n",
            "Finished creating img files for OR021@6_2.mat\n",
            "\n",
            "X212_DE_time\n",
            "121991\n",
            "IR021_3 train\n",
            "Total number of train_images  : 769\n",
            "train images number in folder:29222\n",
            "Finished creating img files for IR021_3.mat\n",
            "\n",
            "X247_DE_time\n",
            "121991\n",
            "OR021@3_1 val\n",
            "Total number of val_images  : 192\n",
            "Finished creating img files for OR021@3_1.mat\n",
            "\n",
            "X210_DE_time\n",
            "121556\n",
            "IR021_1 val\n",
            "Total number of val_images  : 192\n",
            "Finished creating img files for IR021_1.mat\n",
            "\n",
            "X248_DE_time\n",
            "122281\n",
            "OR021@3_2 test\n",
            "Total number of test_images  : 192\n",
            "Finished creating img files for OR021@3_2.mat\n",
            "\n",
            "X108_DE_time\n",
            "122917\n",
            "IR007_3 train\n",
            "Total number of train_images  : 769\n",
            "train images number in folder:29991\n",
            "Finished creating img files for IR007_3.mat\n",
            "\n",
            "X260_DE_time\n",
            "122716\n",
            "OR021@12_2 test\n",
            "Total number of test_images  : 192\n",
            "Finished creating img files for OR021@12_2.mat\n",
            "\n",
            "X186_DE_time\n",
            "122136\n",
            "B014_1 val\n",
            "Total number of val_images  : 192\n",
            "Finished creating img files for B014_1.mat\n",
            "\n",
            "X225_DE_time\n",
            "122136\n",
            "B021_3 train\n",
            "Total number of train_images  : 769\n",
            "train images number in folder:30760\n",
            "Finished creating img files for B021_3.mat\n",
            "\n",
            "X106_DE_time\n",
            "121991\n",
            "IR007_1 val\n",
            "Total number of val_images  : 192\n",
            "Finished creating img files for IR007_1.mat\n",
            "\n",
            "X105_DE_time\n",
            "121265\n",
            "IR007_0 train\n",
            "Total number of train_images  : 769\n",
            "train images number in folder:31529\n",
            "Finished creating img files for IR007_0.mat\n",
            "\n",
            "X160_DE_time\n",
            "122136\n",
            "OR007@12_3 train\n",
            "Total number of train_images  : 769\n",
            "train images number in folder:32298\n",
            "Finished creating img files for OR007@12_3.mat\n",
            "\n",
            "X246_DE_time\n",
            "121701\n",
            "OR021@3_0 train\n",
            "Total number of train_images  : 769\n",
            "train images number in folder:33067\n",
            "Finished creating img files for OR021@3_0.mat\n",
            "\n",
            "X249_DE_time\n",
            "122136\n",
            "OR021@3_3 train\n",
            "Total number of train_images  : 769\n",
            "train images number in folder:33836\n",
            "Finished creating img files for OR021@3_3.mat\n",
            "\n",
            "X209_DE_time\n",
            "122136\n",
            "IR021_0 train\n",
            "Total number of train_images  : 769\n",
            "train images number in folder:34605\n",
            "Finished creating img files for IR021_0.mat\n",
            "\n",
            "X261_DE_time\n",
            "121701\n",
            "OR021@12_3 train\n",
            "Total number of train_images  : 769\n",
            "train images number in folder:35374\n",
            "Finished creating img files for OR021@12_3.mat\n",
            "\n",
            "X131_DE_time\n",
            "122426\n",
            "OR007@6_1 val\n",
            "Total number of val_images  : 192\n",
            "Finished creating img files for OR007@6_1.mat\n",
            "\n",
            "X258_DE_time\n",
            "121846\n",
            "OR021@12_0 train\n",
            "Total number of train_images  : 769\n",
            "train images number in folder:36143\n",
            "Finished creating img files for OR021@12_0.mat\n",
            "\n",
            "X200_DE_time\n",
            "121991\n",
            "OR014@6_3 train\n",
            "Total number of train_images  : 769\n",
            "train images number in folder:36912\n",
            "Finished creating img files for OR014@6_3.mat\n",
            "\n",
            "X235_DE_time\n",
            "121991\n",
            "OR021@6_1 val\n",
            "Total number of val_images  : 192\n",
            "Finished creating img files for OR021@6_1.mat\n",
            "\n",
            "X170_DE_time\n",
            "121846\n",
            "IR014_1 val\n",
            "Total number of val_images  : 192\n",
            "Finished creating img files for IR014_1.mat\n",
            "\n",
            "X172_DE_time\n",
            "121701\n",
            "IR014_3 train\n",
            "Total number of train_images  : 769\n",
            "train images number in folder:37681\n",
            "Finished creating img files for IR014_3.mat\n",
            "\n",
            "X119_DE_time\n",
            "121410\n",
            "B007_1 val\n",
            "Total number of val_images  : 192\n",
            "Finished creating img files for B007_1.mat\n",
            "\n",
            "X118_DE_time\n",
            "122571\n",
            "B007_0 train\n",
            "Total number of train_images  : 769\n",
            "train images number in folder:38450\n",
            "Finished creating img files for B007_0.mat\n",
            "\n",
            "X187_DE_time\n",
            "121991\n",
            "B014_2 test\n",
            "Total number of test_images  : 192\n",
            "Finished creating img files for B014_2.mat\n",
            "\n",
            "X132_DE_time\n",
            "121410\n",
            "OR007@6_2 test\n",
            "Total number of test_images  : 192\n",
            "Finished creating img files for OR007@6_2.mat\n",
            "- - - - - - - - - - - - - - - - - - - -\n",
            "38450\n"
          ]
        }
      ],
      "source": [
        "import scipy.io\n",
        "import random\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import cv2\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import image\n",
        "from pyts.image import GramianAngularField\n",
        "import time\n",
        "import math\n",
        "from pyts.image import MarkovTransitionField\n",
        "# def folder2img(save_path, folder_path, train_num, val_num, test_num, img_size):\n",
        "def folder2img(save_path, folder_path, train_num, val_num, test_num):\n",
        "    files = os.listdir(folder_path)\n",
        "    print(f'\\n{len(files)}')\n",
        "    if not os.path.exists(save_path + '/train'):\n",
        "        os.makedirs(save_path + '/train')\n",
        "    if not os.path.exists(save_path + '/val'):\n",
        "        os.makedirs(save_path + '/val')\n",
        "    if not os.path.exists(save_path + '/test'):\n",
        "        os.makedirs(save_path + '/test')\n",
        "    for file in files:\n",
        "        \n",
        "     if os.path.splitext(file)[1] == '.mat':\n",
        "          file_path = folder_path + \"/\" + file\n",
        "          raw_data = scipy.io.loadmat(file_path)\n",
        "          for name in raw_data.keys():\n",
        "              if \"DE\" in name:\n",
        "                  column_DE = name\n",
        "                  print(f'\\n{column_DE}')\n",
        "          data_DE = raw_data[column_DE]\n",
        "          \n",
        "          n_sample,_, = data_DE.shape\n",
        "          print(n_sample)\n",
        "          final_data=data_DE\n",
        "            \n",
        "          # im_size = 128\n",
        "          series = 1000\n",
        "          # step = 200  # step of slide window\n",
        "          ss = n_sample - series\n",
        "          # im_sum = (n_sample - im_size) // step\n",
        "          # gasf = GramianAngularField(image_size=im_size, method='difference')\n",
        "          mtf = MarkovTransitionField(image_size=0.2, n_bins = 8, strategy = 'quantile')\n",
        "          if (\"_3\" in os.path.splitext(file)[0]) or (\"_0\" in os.path.splitext(file)[0]):\n",
        "                spacing_len = ss //  train_num \n",
        "                choice = list(range(0, train_num*spacing_len, spacing_len))\n",
        "                # choices = [i for i in range(data_DE.shape[0] - im_size)]\n",
        "                print(os.path.splitext(file)[0], \"train\")\n",
        "                # choice = random.sample(choices ,train_num)\n",
        "                \n",
        "                count=0\n",
        "                for i in choice :\n",
        "                # for i in range(im_sum):\n",
        "                  \n",
        "                  # window_size =10\n",
        "                  # start_index, end_index = i  , i + im_size * window_size\n",
        "                  start_index, end_index = i , i + series\n",
        "                  # print(start_index, end_index)\n",
        "                  \n",
        "                  sub_series = final_data[start_index:end_index,:].reshape(1,-1)\n",
        "                  # paa = PiecewiseAggregateApproximation(window_size=window_size)\n",
        "                  # X_paa = paa.transform(sub_series)\n",
        "              \n",
        "                  # print(sub_series)\n",
        "                  # print(sub_series.shape)  #(1,128)\n",
        "                  # GAF_gasf = gasf.fit_transform(sub_series)\n",
        "                  X_mtf = mtf.fit_transform(sub_series)\n",
        "                  im = X_mtf[0]\n",
        "                  # plt.imshow(X_mtf[0], cmap='rainbow', origin='lower')\n",
        "                  # plt.colorbar(fraction=0.0457, pad=0.04)\n",
        "                  im=im*255\n",
        "                  \n",
        "                  # filename = save_path + '%d.png' % i\n",
        "                  # image.imsave(filename, im)\n",
        "                  cv2.imwrite(save_path + '/train/' + os.path.splitext(file)[0] + '_' + str(i) + '-M' + '.png', im)\n",
        "                  count+=1\n",
        "                print(f'Total number of train_images  : {count}')  # 输出总数\n",
        "                path='/content/12kDriveEnd_img0.2/train'\n",
        "                counter = 0\n",
        "                for root,dirs,files in os.walk(path):    #遍历统计\n",
        "                   for each in files:\n",
        "                     counter += 1   #统计文件夹下文件个数\n",
        "                print(f'train images number in folder:{counter}')               #输出结果\n",
        "          elif (\"_1\") in os.path.splitext(file)[0]:\n",
        "                spacing_len = ss //  val_num \n",
        "                choice = list(range(0, val_num*spacing_len, spacing_len))\n",
        "                # choices = [i for i in range(data_DE.shape[0] - im_size)]\n",
        "                print(os.path.splitext(file)[0], \"val\")\n",
        "                # choice = random.sample(choices, val_num)\n",
        "                count=0\n",
        "                for i in choice:\n",
        "                  \n",
        "                  # window_size =10\n",
        "                  # start_index, end_index = i  , i + im_size * window_size\n",
        "                  start_index, end_index = i , i + series\n",
        "                  sub_series = final_data[start_index:end_index,:].reshape(1,-1)\n",
        "                  # paa = PiecewiseAggregateApproximation(window_size=window_size)\n",
        "                  # X_paa = paa.transform(sub_series)\n",
        "                  \n",
        "                  \n",
        "                  # sub_series = final_data[start_index:end_index,:].reshape(1,-1)\n",
        "                  # GAF_gasf = gasf.fit_transform(sub_series)\n",
        "                  X_mtf = mtf.fit_transform(sub_series)\n",
        "                  im = X_mtf[0]\n",
        "                  im=im*255\n",
        "                  cv2.imwrite(save_path + '/val/' + os.path.splitext(file)[0] + '_' + str(i) + '-M' + '.png', im)\n",
        "                  count+=1\n",
        "                print(f'Total number of val_images  : {count}')  # 输出总数\n",
        "                \n",
        "          elif (\"_2\") in os.path.splitext(file)[0]:\n",
        "                spacing_len = ss //  test_num \n",
        "                choice = list(range(0, test_num*spacing_len, spacing_len))\n",
        "                # choices = [i for i in range(data_DE.shape[0] - im_size)]\n",
        "                print(os.path.splitext(file)[0], \"test\")\n",
        "                # choice = random.sample(choices, test_num)\n",
        "                count=0\n",
        "                for i in choice:\n",
        "                  # window_size =10\n",
        "                  # start_index, end_index = i  , i + im_size * window_size\n",
        "                  start_index, end_index = i , i + series\n",
        "                  \n",
        "                  # sub_series = final_data[:, start_index:end_index]\n",
        "                  sub_series = final_data[start_index:end_index,:].reshape(1,-1)\n",
        "                  # paa = PiecewiseAggregateApproximation(window_size=window_size)\n",
        "                  # X_paa = paa.transform(sub_series)\n",
        "                  # print(sub_series.shape)\n",
        "                  # GAF_gasf = gasf.fit_transform(sub_series)\n",
        "                  X_mtf = mtf.fit_transform(sub_series)\n",
        "                  im = X_mtf[0]\n",
        "                  im=im*255\n",
        "                  cv2.imwrite(save_path + '/test/' + os.path.splitext(file)[0] + '_' + str(i) + '-M' + '.png', im)\n",
        "                  count+=1\n",
        "                print(f'Total number of test_images  : {count}')  # 输出总数  \n",
        "          else:\n",
        "                print(\"wrong!\", os.path.splitext(file)[0])\n",
        "\n",
        "          print('Finished creating img files for ' + file)\n",
        "    print('- - - - - - - - - - - - - - - - - - - -')\n",
        "data_path = '/content/Motor_Bearing_Fault_Diagnosis/data/12kDriveEnd'\n",
        "normal_path = '/content/Motor_Bearing_Fault_Diagnosis/data/Normal_Baseline_Data'\n",
        "img_save_path = '/content/12kDriveEnd_img0.2'\n",
        "train_number = round(20000/26)  # Real train number/26\n",
        "val_number = round(2500/13)  # Real val number/13\n",
        "test_number = round(2500/13)  # Real test number/13\n",
        "# matrix_size =128 * 128\n",
        "\n",
        "# folder2img(img_save_path, data_path, train_number, val_number, test_number, matrix_size)\n",
        "folder2img(img_save_path, data_path, train_number, val_number, test_number)\n",
        "import os\n",
        "\n",
        "path='/content/12kDriveEnd_img0.2/train'\n",
        "count = 0\n",
        "for root,dirs,files in os.walk(path):    #遍历统计\n",
        "      for each in files:\n",
        "             count += 1   #统计文件夹下文件个数\n",
        "print(count)              #输出结果"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tOkvMUwj9a0e"
      },
      "source": [
        "#Normal signals transformed into imgs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qkY162B99p6W"
      },
      "source": [
        "##GAF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "furdwI1S9uEf",
        "outputId": "02e2cef1-17f2-4256-9c49-b892977c8c38"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " 4\n",
            "\n",
            "X097_DE_time\n",
            "243938\n",
            "NORMAL_0 train\n",
            "Total number of train_images numbers : 1111\n",
            "train images number in folder:0\n",
            "Finished creating img files for NORMAL_0.mat\n",
            "\n",
            "X099_DE_time\n",
            "485063\n",
            "NORMAL_2 test\n",
            "Total number of test_images numbers : 278\n",
            "Finished creating img files for NORMAL_2.mat\n",
            "\n",
            "X098_DE_time\n",
            "483903\n",
            "NORMAL_1 val\n",
            "Total number of val_images numbers : 278\n",
            "Finished creating img files for NORMAL_1.mat\n",
            "\n",
            "X100_DE_time\n",
            "485643\n",
            "NORMAL_3 train\n",
            "Total number of train_images numbers : 1111\n",
            "train images number in folder:0\n",
            "Finished creating img files for NORMAL_3.mat\n",
            "- - - - - - - - - - - - - - - - - - - -\n",
            "40672\n"
          ]
        }
      ],
      "source": [
        "import scipy.io\n",
        "import random\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import cv2\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import image\n",
        "from pyts.image import GramianAngularField\n",
        "import time\n",
        "import math\n",
        "# def folder2img(save_path, folder_path, train_num, val_num, test_num, img_size):\n",
        "def folder2img(save_path, folder_path, train_num, val_num, test_num):\n",
        "    files = os.listdir(folder_path)\n",
        "    print(f'\\n {len(files)}')\n",
        "    if not os.path.exists(save_path + '/train'):\n",
        "        os.makedirs(save_path + '/train')\n",
        "    if not os.path.exists(save_path + '/val'):\n",
        "        os.makedirs(save_path + '/val')\n",
        "    if not os.path.exists(save_path + '/test'):\n",
        "        os.makedirs(save_path + '/test')\n",
        "    for file in files:\n",
        "        \n",
        "     if os.path.splitext(file)[1] == '.mat':\n",
        "          file_path = folder_path + \"/\" + file\n",
        "          raw_data = scipy.io.loadmat(file_path)\n",
        "          for name in raw_data.keys():\n",
        "              if \"DE\" in name:\n",
        "                  column_DE = name\n",
        "                  print(f'\\n{column_DE}')\n",
        "          data_DE = raw_data[column_DE]\n",
        "          \n",
        "          n_sample,_, = data_DE.shape\n",
        "          print(n_sample)\n",
        "          final_data=data_DE\n",
        "            \n",
        "          # im_size = 128\n",
        "\n",
        "          series = 1000   # each time series\n",
        "\n",
        "          # im_size = 200\n",
        "          # step = 1000  # step of slide window\n",
        "          \n",
        "          ss = n_sample - series\n",
        "          # im_sum = (n_sample - im_size) // step\n",
        "          gasf = GramianAngularField(image_size=0.2, method='difference')\n",
        "          if (\"_3\" in os.path.splitext(file)[0]) or (\"_0\" in os.path.splitext(file)[0]):\n",
        "                # spacing_len = （signal_len - im_size） //  train_num   报错无效字符，不能用小括号吗\n",
        "                spacing_len = ss //  train_num\n",
        "                choice = list(range(0, train_num*spacing_len, spacing_len))\n",
        "                # choices = [i for i in range(data_DE.shape[0] - im_size)]  \n",
        "                print(os.path.splitext(file)[0], \"train\")              \n",
        "                # choice = random.sample(choices ,train_num)\n",
        "                \n",
        "                count=0\n",
        "                for i in choice :\n",
        "                # for i in range(im_sum):\n",
        "                  start_number = i\n",
        "                  # start_index, end_index = i * step, i * step + im_size\n",
        "                  start_index, end_index = i , i + series\n",
        "                  # print(start_index, end_index)\n",
        "                  \n",
        "                  sub_series = final_data[start_index:end_index,:].reshape(1,-1)\n",
        "              \n",
        "                  # print(sub_series)\n",
        "                  # print(sub_series.shape)  #(1,128)\n",
        "                  GAF_gasf = gasf.fit_transform(sub_series)\n",
        "                  im = GAF_gasf[0]\n",
        "                  im=im*255\n",
        "                  \n",
        "                  # filename = save_path + '%d.png' % i\n",
        "                  # image.imsave(filename, im)\n",
        "                  cv2.imwrite(save_path + '/train/' + os.path.splitext(file)[0] + '_' + str(i) + '-G' + '.png', im)\n",
        "                  count+=1\n",
        "                print(f'Total number of train_images numbers : {count}')  # 输出总数\n",
        "                path='/content/12kDriveEnd_img/train'\n",
        "                counter = 0\n",
        "                for root,dirs,files in os.walk(path):    #遍历统计\n",
        "                   for each in files:\n",
        "                     counter += 1   #统计文件夹下文件个数\n",
        "                print(f'train images number in folder:{counter}')               #输出结果\n",
        "          elif (\"_1\") in os.path.splitext(file)[0]:\n",
        "                spacing_len = ss //  val_num \n",
        "                choice = list(range(0, val_num*spacing_len, spacing_len))\n",
        "                # choices = [i for i in range(data_DE.shape[0] - im_size)]\n",
        "                print(os.path.splitext(file)[0], \"val\")\n",
        "                # choice = random.sample(choices, val_num)\n",
        "                count=0\n",
        "                for i in choice:\n",
        "                  start_number = i\n",
        "                  start_index, end_index = i , i + series\n",
        "                  \n",
        "                  \n",
        "                  sub_series = final_data[start_index:end_index,:].reshape(1,-1)\n",
        "                  GAF_gasf = gasf.fit_transform(sub_series)\n",
        "                  im = GAF_gasf[0]\n",
        "                  im=im*255\n",
        "                  cv2.imwrite(save_path + '/val/' + os.path.splitext(file)[0] + '_' + str(i) + '-G' + '.png', im)\n",
        "                  count+=1\n",
        "                print(f'Total number of val_images numbers : {count}')  # 输出总数\n",
        "                \n",
        "          elif (\"_2\") in os.path.splitext(file)[0]:\n",
        "                spacing_len = ss //  test_num \n",
        "                choice = list(range(0, test_num*spacing_len, spacing_len))\n",
        "                # choices = [i for i in range(data_DE.shape[0] - im_size)]\n",
        "                print(os.path.splitext(file)[0], \"test\")\n",
        "                # choice = random.sample(choices, test_num)\n",
        "                count=0\n",
        "                for i in choice:\n",
        "                  start_index, end_index = i , i + series\n",
        "                  \n",
        "                  # sub_series = final_data[:, start_index:end_index]\n",
        "                  sub_series = final_data[start_index:end_index,:].reshape(1,-1)\n",
        "                  # print(sub_series.shape)\n",
        "                  GAF_gasf = gasf.fit_transform(sub_series)\n",
        "                  im = GAF_gasf[0]\n",
        "                  im=im*255\n",
        "                  cv2.imwrite(save_path + '/test/' + os.path.splitext(file)[0] + '_' + str(i) + '-G' + '.png', im)\n",
        "                  count+=1\n",
        "                print(f'Total number of test_images numbers : {count}')  # 输出总数  \n",
        "          else:\n",
        "                print(\"wrong!\", os.path.splitext(file)[0])\n",
        "\n",
        "          print('Finished creating img files for ' + file)\n",
        "    print('- - - - - - - - - - - - - - - - - - - -')\n",
        "data_path = '/content/Motor_Bearing_Fault_Diagnosis/data/12kDriveEnd'\n",
        "normal_path = '/content/Motor_Bearing_Fault_Diagnosis/data/Normal_Baseline_Data'\n",
        "img_save_path = '/content/12kDriveEnd_img0.2'\n",
        "train_number = round(20000/2/9)  # Real train number/2\n",
        "val_number = round(2500/9)   # Real val number\n",
        "test_number = round(2500/9)  # Real test number\n",
        "# matrix_size =128 * 128\n",
        "\n",
        "# folder2img(img_save_path, data_path, train_number, val_number, test_number, matrix_size)\n",
        "folder2img(img_save_path, normal_path, train_number, val_number, test_number)\n",
        "import os\n",
        "\n",
        "path='/content/12kDriveEnd_img0.2/train'\n",
        "count = 0\n",
        "for root,dirs,files in os.walk(path):    #遍历统计\n",
        "      for each in files:\n",
        "             count += 1   #统计文件夹下文件个数\n",
        "print(count)              #输出结果"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9TMsq4WG-KMf"
      },
      "source": [
        "##MAR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xf-HXwK5-Ny-",
        "outputId": "c588a9ed-8710-4b65-87a4-7568e50abd0c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "4\n",
            "\n",
            "X097_DE_time\n",
            "243938\n",
            "NORMAL_0 train\n",
            "Total number of train_images numbers : 1111\n",
            "train images number in folder:0\n",
            "Finished creating img files for NORMAL_0.mat\n",
            "\n",
            "X099_DE_time\n",
            "485063\n",
            "NORMAL_2 test\n",
            "Total number of test_images numbers : 278\n",
            "Finished creating img files for NORMAL_2.mat\n",
            "\n",
            "X098_DE_time\n",
            "483903\n",
            "NORMAL_1 val\n",
            "Total number of val_images numbers : 278\n",
            "Finished creating img files for NORMAL_1.mat\n",
            "\n",
            "X100_DE_time\n",
            "485643\n",
            "NORMAL_3 train\n",
            "Total number of train_images numbers : 1111\n",
            "train images number in folder:0\n",
            "Finished creating img files for NORMAL_3.mat\n",
            "- - - - - - - - - - - - - - - - - - - -\n",
            "42894\n"
          ]
        }
      ],
      "source": [
        "import scipy.io\n",
        "import random\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import cv2\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import image\n",
        "from pyts.image import GramianAngularField\n",
        "import time\n",
        "import math\n",
        "from pyts.image import MarkovTransitionField\n",
        "# def folder2img(save_path, folder_path, train_num, val_num, test_num, img_size):\n",
        "def folder2img(save_path, folder_path, train_num, val_num, test_num):\n",
        "    files = os.listdir(folder_path)\n",
        "    print(f'\\n{len(files)}')\n",
        "    if not os.path.exists(save_path + '/train'):\n",
        "        os.makedirs(save_path + '/train')\n",
        "    if not os.path.exists(save_path + '/val'):\n",
        "        os.makedirs(save_path + '/val')\n",
        "    if not os.path.exists(save_path + '/test'):\n",
        "        os.makedirs(save_path + '/test')\n",
        "    for file in files:\n",
        "        \n",
        "     if os.path.splitext(file)[1] == '.mat':\n",
        "          file_path = folder_path + \"/\" + file\n",
        "          raw_data = scipy.io.loadmat(file_path)\n",
        "          for name in raw_data.keys():\n",
        "              if \"DE\" in name:\n",
        "                  column_DE = name\n",
        "                  print(f'\\n{column_DE}')\n",
        "          data_DE = raw_data[column_DE]\n",
        "          \n",
        "          n_sample,_, = data_DE.shape\n",
        "          print(n_sample)\n",
        "          final_data=data_DE\n",
        "            \n",
        "          # im_size = 128\n",
        "          series = 1000\n",
        "          # step = 200  # step of slide window\n",
        "          ss = n_sample - series\n",
        "          # im_sum = (n_sample - im_size) // step\n",
        "          # gasf = GramianAngularField(image_size=im_size, method='difference')\n",
        "          mtf = MarkovTransitionField(image_size=0.2, n_bins = 10, strategy = 'quantile')\n",
        "          if (\"_3\" in os.path.splitext(file)[0]) or (\"_0\" in os.path.splitext(file)[0]):\n",
        "                spacing_len = ss //  train_num \n",
        "                choice = list(range(0, train_num*spacing_len, spacing_len))\n",
        "                # choices = [i for i in range(data_DE.shape[0] - im_size)]\n",
        "                print(os.path.splitext(file)[0], \"train\")\n",
        "                # choice = random.sample(choices ,train_num)\n",
        "                \n",
        "                count=0\n",
        "                for i in choice :\n",
        "                # for i in range(im_sum):\n",
        "                  \n",
        "                  # window_size =10\n",
        "                  # start_index, end_index = i  , i + im_size * window_size\n",
        "                  start_index, end_index = i , i + series\n",
        "                  # print(start_index, end_index)\n",
        "                  \n",
        "                  sub_series = final_data[start_index:end_index,:].reshape(1,-1)\n",
        "                  # paa = PiecewiseAggregateApproximation(window_size=window_size)\n",
        "                  # X_paa = paa.transform(sub_series)\n",
        "              \n",
        "                  # print(sub_series)\n",
        "                  # print(sub_series.shape)  #(1,128)\n",
        "                  # GAF_gasf = gasf.fit_transform(sub_series)\n",
        "                  X_mtf = mtf.fit_transform(sub_series)\n",
        "                  im = X_mtf[0]\n",
        "                  im=im*255\n",
        "                  \n",
        "                  # filename = save_path + '%d.png' % i\n",
        "                  # image.imsave(filename, im)\n",
        "                  cv2.imwrite(save_path + '/train/' + os.path.splitext(file)[0] + '_' + str(i) + '-M' + '.png', im)\n",
        "                  count+=1\n",
        "                print(f'Total number of train_images numbers : {count}')  # 输出总数\n",
        "                path='/content/12kDriveEnd_img/train'\n",
        "                counter = 0\n",
        "                for root,dirs,files in os.walk(path):    #遍历统计\n",
        "                   for each in files:\n",
        "                     counter += 1   #统计文件夹下文件个数\n",
        "                print(f'train images number in folder:{counter}')               #输出结果\n",
        "          elif (\"_1\") in os.path.splitext(file)[0]:\n",
        "                spacing_len = ss //  val_num \n",
        "                choice = list(range(0, val_num*spacing_len, spacing_len))\n",
        "                # choices = [i for i in range(data_DE.shape[0] - im_size)]\n",
        "                print(os.path.splitext(file)[0], \"val\")\n",
        "                # choice = random.sample(choices, val_num)\n",
        "                count=0\n",
        "                for i in choice:\n",
        "                  \n",
        "                  # window_size =10\n",
        "                  # start_index, end_index = i  , i + im_size * window_size\n",
        "                  start_index, end_index = i , i + series\n",
        "                  sub_series = final_data[start_index:end_index,:].reshape(1,-1)\n",
        "                  # paa = PiecewiseAggregateApproximation(window_size=window_size)\n",
        "                  # X_paa = paa.transform(sub_series)\n",
        "                  \n",
        "                  \n",
        "                  # sub_series = final_data[start_index:end_index,:].reshape(1,-1)\n",
        "                  # GAF_gasf = gasf.fit_transform(sub_series)\n",
        "                  X_mtf = mtf.fit_transform(sub_series)\n",
        "                  im = X_mtf[0]\n",
        "                  im=im*255\n",
        "                  cv2.imwrite(save_path + '/val/' + os.path.splitext(file)[0] + '_' + str(i) + '-M' + '.png', im)\n",
        "                  count+=1\n",
        "                print(f'Total number of val_images numbers : {count}')  # 输出总数\n",
        "                \n",
        "          elif (\"_2\") in os.path.splitext(file)[0]:\n",
        "                spacing_len = ss //  test_num \n",
        "                choice = list(range(0, test_num*spacing_len, spacing_len))\n",
        "                # choices = [i for i in range(data_DE.shape[0] - im_size)]\n",
        "                print(os.path.splitext(file)[0], \"test\")\n",
        "                # choice = random.sample(choices, test_num)\n",
        "                count=0\n",
        "                for i in choice:\n",
        "                  # window_size =10\n",
        "                  # start_index, end_index = i  , i + im_size * window_size\n",
        "                  start_index, end_index = i , i + series\n",
        "                  \n",
        "                  # sub_series = final_data[:, start_index:end_index]\n",
        "                  sub_series = final_data[start_index:end_index,:].reshape(1,-1)\n",
        "                  # paa = PiecewiseAggregateApproximation(window_size=window_size)\n",
        "                  # X_paa = paa.transform(sub_series)\n",
        "                  # print(sub_series.shape)\n",
        "                  # GAF_gasf = gasf.fit_transform(sub_series)\n",
        "                  X_mtf = mtf.fit_transform(sub_series)\n",
        "                  im = X_mtf[0]\n",
        "                  im=im*255\n",
        "                  cv2.imwrite(save_path + '/test/' + os.path.splitext(file)[0] + '_' + str(i) + '-M' + '.png', im)\n",
        "                  count+=1\n",
        "                print(f'Total number of test_images numbers : {count}')  # 输出总数  \n",
        "          else:\n",
        "                print(\"wrong!\", os.path.splitext(file)[0])\n",
        "\n",
        "          print('Finished creating img files for ' + file)\n",
        "    print('- - - - - - - - - - - - - - - - - - - -')\n",
        "data_path = '/content/Motor_Bearing_Fault_Diagnosis/data/12kDriveEnd'\n",
        "normal_path = '/content/Motor_Bearing_Fault_Diagnosis/data/Normal_Baseline_Data'\n",
        "img_save_path = '/content/12kDriveEnd_img0.2'\n",
        "train_number = round(20000/2/9)  # Real train number/2\n",
        "val_number = round(2500/9)   # Real val number\n",
        "test_number = round(2500/9)  # Real test number\n",
        "# matrix_size =128 * 128\n",
        "\n",
        "# folder2img(img_save_path, data_path, train_number, val_number, test_number, matrix_size)\n",
        "folder2img(img_save_path, normal_path, train_number, val_number, test_number)\n",
        "import os\n",
        "\n",
        "path='/content/12kDriveEnd_img0.2/train'\n",
        "count = 0\n",
        "for root,dirs,files in os.walk(path):    #遍历统计\n",
        "      for each in files:\n",
        "             count += 1   #统计文件夹下文件个数\n",
        "print(count)              #输出结果"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pyC74y2TthHq"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jHNYZFYvFim_"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UzdE1uo5jHOz"
      },
      "source": [
        "## 获取原始文件路径list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "RAupeIjveHI7"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "train_filePath = '/content/12kDriveEnd_img0.2/train'\n",
        "train_file_path_list_ori = os.listdir(train_filePath)\n",
        "val_filePath = '/content/12kDriveEnd_img0.2/val'\n",
        "val_file_path_list_ori = os.listdir(val_filePath)\n",
        "test_filePath = '/content/12kDriveEnd_img0.2/test'\n",
        "test_file_path_list_ori = os.listdir(test_filePath)\n",
        "\n",
        "# file_path_list_ori[:5]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-tnriOqljMEu"
      },
      "source": [
        "## 转化为文件名list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "uOXysrSAiPpF"
      },
      "outputs": [],
      "source": [
        "train_file_name_list = list(set([file_path.split('-')[0] for file_path in train_file_path_list_ori]))\n",
        "val_file_name_list = list(set([file_path.split('-')[0] for file_path in val_file_path_list_ori]))\n",
        "test_file_name_list = list(set([file_path.split('-')[0] for file_path in test_file_path_list_ori]))\n",
        "# file_name_list[:5],len(file_name_list)\n",
        "# file_name_list,len(file_name_list)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rRdXlfJsldsM"
      },
      "source": [
        "## 构建名称与label id的映射关系"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LfxPQM9rO4xP",
        "outputId": "64c769a3-3cec-45e1-b479-4d2b6a8bb2d3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "def get_label_num(file_name):\n",
        "  if 'B007' in file_name:\n",
        "    return 3\n",
        "  elif 'B014' in file_name:\n",
        "    return 6\n",
        "  elif 'B021' in file_name:\n",
        "    return 9\n",
        "  elif 'OR007' in file_name:\n",
        "    return 1\n",
        "  elif 'OR014' in file_name:\n",
        "    return 4\n",
        "  elif 'OR021' in file_name:\n",
        "    return 7\n",
        "  elif 'IR007' in file_name:\n",
        "    return 2\n",
        "  elif 'IR014' in file_name:\n",
        "    return 5\n",
        "  elif 'IR021' in file_name:\n",
        "    return 8\n",
        "  else :\n",
        "    return 0\n",
        "get_label_num('OR1007@12_3_60568')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3q2hk_38lkew"
      },
      "source": [
        "## 构建dataset类"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "TlajsrXHnLrN"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler\n",
        "import torch.backends.cudnn as cudnn\n",
        "import numpy as np\n",
        "import torchvision\n",
        "from torchvision import datasets, models, transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uEHdvlUxmo4h"
      },
      "outputs": [],
      "source": [
        "img_transform = transforms.Compose([\n",
        "        transforms.Resize(200),\n",
        "        # transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor()])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k7Ih-FQqeHPm",
        "outputId": "77c9c961-e624-481f-c2a8-1e22a9cae0b6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((tensor([[[40, 36, 31,  ..., 40, 36, 37],\n",
              "           [38, 38, 37,  ..., 37, 36, 36],\n",
              "           [37, 37, 37,  ..., 36, 34, 35],\n",
              "           ...,\n",
              "           [39, 37, 33,  ..., 39, 37, 37],\n",
              "           [37, 35, 33,  ..., 37, 36, 36],\n",
              "           [38, 36, 34,  ..., 38, 37, 36]]], dtype=torch.uint8),\n",
              "  tensor([[[40, 36, 31,  ..., 40, 36, 37],\n",
              "           [38, 38, 37,  ..., 37, 36, 36],\n",
              "           [37, 37, 37,  ..., 36, 34, 35],\n",
              "           ...,\n",
              "           [39, 37, 33,  ..., 39, 37, 37],\n",
              "           [37, 35, 33,  ..., 37, 36, 36],\n",
              "           [38, 36, 34,  ..., 38, 37, 36]]], dtype=torch.uint8)),\n",
              " 1)"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from torchvision.io import read_image\n",
        "from PIL import Image\n",
        "\n",
        "train_img_dir = '/content/12kDriveEnd_img0.2/train'\n",
        "val_img_dir = '/content/12kDriveEnd_img0.2/val'\n",
        "test_img_dir = '/content/12kDriveEnd_img0.2/test'\n",
        "class CustomImageDataset(Dataset):\n",
        "    # def __init__(self, file_name_list, img_dir,img_transform):\n",
        "    def __init__(self, file_name_list, img_dir):\n",
        "        self.file_name_list = file_name_list\n",
        "        self.img_dir = img_dir\n",
        "        # self.img_transform = img_transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.file_name_list)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        file_name = self.file_name_list[idx]\n",
        "        img_path_m = os.path.join(self.img_dir, self.file_name_list[idx]+'-M.png')\n",
        "        img_path_g = os.path.join(self.img_dir, self.file_name_list[idx]+'-G.png')\n",
        "        image_m = read_image(img_path_m)\n",
        "        image_g = read_image(img_path_m)\n",
        "        # image_m = Image.open(img_path_m)\n",
        "        # image_g = Image.open(img_path_g)\n",
        "        # image_tensor_m = self.img_transform(image_m)\n",
        "        # image_tensor_g = self.img_transform(image_g)\n",
        "        # label_name = file_name.split('_')[0]\n",
        "        # label_id = le.transform([label_name])[0]\n",
        "        label_id = get_label_num(file_name)\n",
        "\n",
        "        return (image_m,image_g), label_id\n",
        "\n",
        "# train_img_dataset = CustomImageDataset( file_name_list, train_img_dir,img_transform)\n",
        "# val_img_dataset = CustomImageDataset( file_name_list, val_img_dir,img_transform)\n",
        "# test_img_dataset = CustomImageDataset( file_name_list, test_img_dir,img_transform)\n",
        "train_img_dataset = CustomImageDataset( train_file_name_list, train_img_dir)\n",
        "val_img_dataset = CustomImageDataset( val_file_name_list, val_img_dir)\n",
        "test_img_dataset = CustomImageDataset( test_file_name_list, test_img_dir)\n",
        "train_img_dataset[2]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7guKJdlvqg3d"
      },
      "source": [
        "## 构建dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "XzDRLxzHpziW"
      },
      "outputs": [],
      "source": [
        "train_dataloader = DataLoader(train_img_dataset, batch_size=64, shuffle=True)\n",
        "val_dataloader = DataLoader(val_img_dataset, batch_size=64, shuffle=True)\n",
        "test_dataloader = DataLoader(test_img_dataset, batch_size=64, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wq19Hs7xYFVJ",
        "outputId": "c9ed088e-8221-481c-bed2-334480f8f888"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "21447"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "len(train_dataloader.dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nl1UWzyURgcl"
      },
      "source": [
        "# 模型"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fbH5bhT7R_qP"
      },
      "source": [
        "## base model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "YXthY6RCp-MM",
        "outputId": "408ce540-b1b7-4754-ca6f-5ec099036a0b"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-5fe89809e6ea>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0mbase_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBase_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0mbase_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'images' is not defined"
          ]
        }
      ],
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class Base_model(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
        "        self.fc1 = nn.Linear(35344, 120)\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, 50)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "base_model = Base_model()\n",
        "base_model(images[0]).shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XauoD2oOWt-Q"
      },
      "source": [
        "## CRNN模型"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "dDzQKpKjUxek"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch\n",
        "\n",
        "class BidirectionalLSTM(nn.Module):\n",
        "    # Inputs hidden units Out\n",
        "    def __init__(self, nIn, nHidden, nOut):\n",
        "        super(BidirectionalLSTM, self).__init__()\n",
        "\n",
        "        self.rnn = nn.LSTM(nIn, nHidden, bidirectional=True)\n",
        "        self.embedding = nn.Linear(nHidden * 2, nOut)\n",
        "\n",
        "    def forward(self, input):\n",
        "        recurrent, _ = self.rnn(input)\n",
        "        T, b, h = recurrent.size()\n",
        "        t_rec = recurrent.view(T * b, h)\n",
        "\n",
        "        output = self.embedding(t_rec)  # [T * b, nOut]\n",
        "        output = output.view(T, b, -1)\n",
        "\n",
        "        return output\n",
        "\n",
        "\n",
        "class CRNN(nn.Module):\n",
        "    # def __init__(self, imgH, nc, nclass, nh, n_rnn=2, leakyRelu=False, phi=0):\n",
        "    def __init__(self, imgH, nc, nclass, nh, n_rnn=2, leakyRelu=False):\n",
        "        super(CRNN, self).__init__()\n",
        "        # assert imgH % 16 == 0, 'imgH has to be a multiple of 16'\n",
        "\n",
        "        ks = [3, 3, 3, 3, 3, 3, 3]\n",
        "        ps = [1, 1, 1, 1, 1, 1, 0]\n",
        "        ss = [1, 1, 1, 1, 1, 1, (3, 1)]\n",
        "        nm = [64, 128, 256, 256, 512, 512, 512]\n",
        "\n",
        "        cnn = nn.Sequential()\n",
        "\n",
        "        # if phi>=1 and phi<=3:\n",
        "        #   self\n",
        "\n",
        "        def convRelu(i, batchNormalization=False):\n",
        "            nIn = nc if i == 0 else nm[i - 1]\n",
        "            nOut = nm[i]\n",
        "            cnn.add_module('conv{0}'.format(i),\n",
        "                           nn.Conv2d(nIn, nOut, ks[i], ss[i], ps[i]))\n",
        "            if batchNormalization:\n",
        "                cnn.add_module('batchnorm{0}'.format(i), nn.BatchNorm2d(nOut))\n",
        "            if leakyRelu:\n",
        "                cnn.add_module('relu{0}'.format(i),\n",
        "                               nn.LeakyReLU(0.2, inplace=True))\n",
        "            else:\n",
        "                cnn.add_module('relu{0}'.format(i), nn.ReLU(True))\n",
        "\n",
        "        convRelu(0)\n",
        "        cnn.add_module('pooling{0}'.format(0), nn.MaxPool2d(2, 2))  # 64*100*100\n",
        "        convRelu(1)\n",
        "        cnn.add_module('pooling{0}'.format(1), nn.MaxPool2d(2, 2))  # 128*50*50\n",
        "        convRelu(2, True)\n",
        "        convRelu(3)\n",
        "        cnn.add_module('pooling{0}'.format(2), nn.MaxPool2d(2, 2))  # 256x25x25\n",
        "        convRelu(4, True)\n",
        "        convRelu(5)\n",
        "\n",
        "        # cnn.add_module('pooling{0}'.format(3), nn.MaxPool2d(2, 2))  # 512*8*11\n",
        "\n",
        "        cnn.add_module('pooling{0}'.format(3),\n",
        "                       nn.MaxPool2d((3, 3), (3, 2), (1, 0)))  # 512*20*20\n",
        "\n",
        "        convRelu(6)  # 512*2*10\n",
        "        # self.Relu6 = convRelu(6)\n",
        "\n",
        "        cnn.add_module('pooling{0}'.format(4),\n",
        "                       nn.MaxPool2d((2, 3), (3, 1), (0, 1)))  # 512*1*10\n",
        "\n",
        "        # #                            # 512*6*6=18432\n",
        "\n",
        "        # self.flatten = nn.Flatten()    #32*18432\n",
        "\n",
        "        # self.fc0 = nn.Linear(18432, 2048)\n",
        "        # self.Relu7 = nn.ReLU(inplace=True)\n",
        "        # self.drop0 = nn.Dropout(p=0.5)\n",
        "        # self.fc1 = nn.Linear(2048, 2048)\n",
        "        # self.Relu8 = nn.ReLU(inplace=True)\n",
        "\n",
        "        # self.to_lstm = nn.Sequential(\n",
        "\n",
        "        #     nn.ReLU(inplace=true),\n",
        "        #     nn.Linear(2048, 10)\n",
        "        # )\n",
        "\n",
        "        self.cnn = cnn\n",
        "        self.rnn = nn.Sequential(\n",
        "            BidirectionalLSTM(512, nh, nh),\n",
        "            BidirectionalLSTM(nh, nh, nclass))\n",
        "        self.flatten = nn.Flatten()  # 32*100\n",
        "\n",
        "        self.fc0 = nn.Linear(100, 10)\n",
        "        self.Relu7 = nn.ReLU(inplace=True)\n",
        "        # self.drop0 = nn.Dropout(p=0.5)\n",
        "        self.fc1 = nn.Linear(2048, 10)\n",
        "        # self.Relu8 = nn.ReLU(inplace=True)\n",
        "        # self.softmax = nn.Softmax(dim=0)\n",
        "\n",
        "    def forward(self, input):\n",
        "\n",
        "        # conv features\n",
        "        conv = self.cnn(input)  # [32, 512, 1, 10]\n",
        "        b, c, h, w = conv.size()\n",
        "\n",
        "        # print('conv:', conv.size())\n",
        "\n",
        "        assert h == 1, \"the height of conv must be 1\"\n",
        "        conv = conv.squeeze(2)  # b *512 * width\n",
        "        conv = conv.permute(2, 0, 1)  # [w, b, c]\n",
        "\n",
        "        rnn_out = self.rnn(conv)  # [10, 32, 4]   new[10, 32, 10]\n",
        "        # print('rnn_out:', rnn_out.size())\n",
        "\n",
        "        rnn_out = rnn_out.permute(1, 0, 2)  # [b, w, c]\n",
        "        y = self.flatten(rnn_out)  # [b, w*c]\n",
        "        # print('flatten:', y.size())\n",
        "        output = self.fc0(y)\n",
        "        # print('fc0:', output.size())\n",
        "     \n",
        "\n",
        "        return output\n",
        "\n",
        "model_crnn=CRNN(200,1,10,64)\n",
        "\n",
        "# model_crnn(images[0]).shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rbCg8HPFS9tM"
      },
      "source": [
        "## 完整模型1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tOxPGqYGTg5Q",
        "outputId": "b0ec45d1-fbd4-4bfd-ae83-f264822f5d80"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([8, 1, 200, 200])"
            ]
          },
          "execution_count": 64,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torch.cat([images[0],images[0]]).shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0NeO4teATBnW",
        "outputId": "36dbdf64-350c-44a3-f125-fb9e182d790c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([4, 10])"
            ]
          },
          "execution_count": 74,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "class Final_model(nn.Module):\n",
        "    def __init__(self):\n",
        "      super().__init__()\n",
        "      self.base_model = base_model\n",
        "      self.fc1 = nn.Linear(100, 50)\n",
        "      self.fc2 = nn.Linear(50, 10)\n",
        "\n",
        "    def forward(self, img_m,img_g):\n",
        "      tensor1 = self.base_model(img_m)\n",
        "      tensor2 = self.base_model(img_g)\n",
        "      mid_tensor = torch.cat([tensor1,tensor2],1)\n",
        "      mid_tensor = self.fc1(mid_tensor)\n",
        "      final_out = self.fc2(mid_tensor)\n",
        "      return final_out\n",
        "final_model_1 = Final_model()\n",
        "final_model_1(images[0],images[1]).shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 169
        },
        "id": "4qY_bGBzYDPg",
        "outputId": "ce484ad5-85b3-442c-9232-d5a226bd80ec"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-30-783c02edd104>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfinal_model_2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_cuda\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'images' is not defined"
          ]
        }
      ],
      "source": [
        "final_model_2(images[0],images[1]).is_cuda"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Xj65AWyYDkK"
      },
      "source": [
        "## 完整模型2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 169
        },
        "id": "eMuS9ZHcYDkK",
        "outputId": "1fdff9a6-74a9-4460-b4cd-5a4aaef8cf69"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-b2a725adc630>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_images\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_images\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'train_images' is not defined"
          ]
        }
      ],
      "source": [
        "torch.cat([train_images[0],train_images[0]]).shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "-a8o9MeFYDkK"
      },
      "outputs": [],
      "source": [
        "class Final_model(nn.Module):\n",
        "    def __init__(self):\n",
        "      super().__init__()\n",
        "      self.base_model = model_crnn\n",
        "      self.fc1 = nn.Linear(20, 10)\n",
        "      # self.fc2 = nn.Linear(100, 10)\n",
        "\n",
        "    def forward(self, img_m,img_g):\n",
        "      tensor1 = self.base_model(img_m)\n",
        "      tensor2 = self.base_model(img_g)\n",
        "      mid_tensor = torch.cat([tensor1,tensor2],1)\n",
        "      # final_out = self.fc1(mid_tensor)\n",
        "      final_out = self.fc1(mid_tensor)\n",
        "      return final_out\n",
        "final_model_2 = Final_model()\n",
        "# final_model_2(images[0],images[1]).shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XaMiGc6YYDS2"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mMdxXkZZYDVu"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v8HmAgr_UxuN"
      },
      "source": [
        "## 分配优化器和损失函数"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 203
        },
        "id": "ufumHok9U0ru",
        "outputId": "007b8187-352c-4d74-e4a9-4a16824122db"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-6ceaba5db5f7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mcriterion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCrossEntropyLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSGD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfinal_model_1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmomentum\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.9\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'final_model_1' is not defined"
          ]
        }
      ],
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(final_model_1.parameters(), lr=0.001, momentum=0.9)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b_shgo8ZU18m"
      },
      "source": [
        "## 构建训练for循环"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dxacrzDfUkXY"
      },
      "outputs": [],
      "source": [
        "for epoch in range(20):  # loop over the dataset multiple times\n",
        "\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(train_dataloader, 0):\n",
        "        # get the inputs; data is a list of [inputs, labels]\n",
        "        (img_m,img_g), labels = data\n",
        "\n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # forward + backward + optimize\n",
        "        outputs = final_model_1(img_m,img_g)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # print statistics\n",
        "        running_loss += loss.item()\n",
        "        if i % 10 == 9:    # print every 2000 mini-batches\n",
        "            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 10:.3f}')\n",
        "            running_loss = 0.0\n",
        "\n",
        "print('Finished Training')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7IFEDmOHFkIR"
      },
      "source": [
        "##原来的训练"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SZKnhqfGFVVj",
        "outputId": "15e70ad7-331e-40ee-d93e-548508bd3ba9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n",
            "learning_rate:  0.003\n",
            "loss: 2.301127  [    0/21447]\n",
            "loss: 2.282361  [ 6400/21447]\n",
            "loss: 2.269553  [12800/21447]\n",
            "loss: 2.273313  [19200/21447]\n",
            "\n",
            " Accuracy: 21.5%, Avg loss: 0.272103 \n",
            "\n",
            "\n",
            " Accuracy: 21.5%, Avg loss: 0.272065 \n",
            "\n",
            "save_model:  0.2151349839138341\n",
            "save_model:  0.2151349839138341\n",
            "done, val_best_score:  0.2151349839138341\n",
            "done, test_best_score:  0.2151349839138341\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "learning_rate:  0.003\n",
            "loss: 2.229097  [    0/21447]\n",
            "loss: 2.219172  [ 6400/21447]\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "# from model import CRNN\n",
        "# from model import BidirectionalLSTM\n",
        "from torch.utils.data import DataLoader\n",
        "# from create_dataset import MyDataset\n",
        "from  torch.utils.tensorboard import SummaryWriter\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR, ExponentialLR\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "from torch.optim.lr_scheduler import MultiStepLR\n",
        "import os\n",
        "import shutil\n",
        "def main():\n",
        "    if os.path.exists('./loss_log'):\n",
        "        shutil.rmtree('./loss_log')\n",
        "    writer = SummaryWriter('./loss_log')\n",
        "    learning_rate = 3e-3\n",
        "    # batch_size = 128\n",
        "    epochs = 400\n",
        "\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    # model = CRNN(200,1,10,256).to(device)\n",
        "    \n",
        "    model = final_model_2.to(device)\n",
        "\n",
        "\n",
        "    # train_data = MyDataset('./train.csv', '/content/12kDriveEnd_img/train')\n",
        "    # eval_data = MyDataset('./val.csv', '/content/12kDriveEnd_img/val')\n",
        "    # test_data = MyDataset('./test.csv', '/content/12kDriveEnd_img/test')\n",
        "    # train_dataloader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "    # eval_dataloader = DataLoader(eval_data, batch_size=batch_size, shuffle=True)\n",
        "    # test_dataloader = DataLoader(test_data, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    loss_fn = nn.CrossEntropyLoss()\n",
        "    base_optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
        "    scheduler = CosineAnnealingLR(base_optimizer, T_max=40, eta_min=5 * 1e-6)\n",
        "    scheduler_1 = ExponentialLR(base_optimizer, gamma=0.95)\n",
        "    scheduler_2 = StepLR(base_optimizer, step_size=3, gamma=0.9)\n",
        "    scheduler_3 = MultiStepLR(\n",
        "    optimizer=base_optimizer,\n",
        "    milestones=[12, 50, 130],  # 设定调整的间隔数\n",
        "    gamma=0.1,  # 系数\n",
        "    last_epoch=-1\n",
        "    )\n",
        "#学习率的问题，loss那么大，一直不提升\n",
        "    def model_test(epoch, dataloader, test_model, highest_score, is_train=True):\n",
        "        size = len(dataloader.dataset)   #test总的文件名个数4174\n",
        "        if not is_train:\n",
        "          test_model.val()\n",
        "        test_loss, correct = 0, 0\n",
        "        total = 0\n",
        "        with torch.no_grad():\n",
        "            # for X, y in dataloader:\n",
        "            for i, data in enumerate(train_dataloader, 0):  #i表示第i个batch_size\n",
        "                (img_m,img_g), labels = data\n",
        "              \n",
        "                img_m  = img_m.float().to(device)\n",
        "                img_g  = img_g.float().to(device)\n",
        "                \n",
        "                outputs = test_model(img_m,img_g)\n",
        "                # the class with the highest energy is what we choose as prediction\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                total += labels.size(0)  #labels.size(0)：分类数10\n",
        "                correct += (predicted == labels.to(device)).sum().item()\n",
        "\n",
        "                test_loss += loss_fn(outputs, labels.to(device)).item()\n",
        "              \n",
        "                # correct += (outputs.argmax(1) == labels.to(device)).type(torch.int).sum().item()\n",
        "                # correct = correct/2\n",
        "        test_loss /= size\n",
        "        # correct /= size\n",
        "        correct /= total\n",
        "        highest_score = max(highest_score, correct)\n",
        "        # print(f'Accuracy of the network on the 10000 test images: {100 * correct // total} %')\n",
        "        # print(f\"\\n Accuracy: {(100 * correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
        "        print(f\"\\n Accuracy: {(100 * correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
        "        niter = epoch * len(train_dataloader)\n",
        "        writer.add_scalars('Train',\n",
        "                           {'val_correct': correct}, niter)\n",
        "        return highest_score\n",
        "\n",
        "    def train_loop(epoch, dataloader, train_model, loss_function, optimizer):\n",
        "        print('learning_rate: ', optimizer.param_groups[0]['lr'])\n",
        "        size = len(dataloader.dataset)\n",
        "\n",
        "        \n",
        "        for i, data in enumerate(train_dataloader, 0):  \n",
        "          # get the inputs; data is a list of [inputs, labels]\n",
        "          (img_m,img_g), labels = data\n",
        "\n",
        "          # zero the parameter gradients\n",
        "          optimizer.zero_grad()\n",
        "\n",
        "          # forward + backward + optimize\n",
        "          img_m  = img_m.float().to(device)\n",
        "          img_g  = img_g.float().to(device)\n",
        "          outputs = final_model_2(img_m,img_g)\n",
        "          loss = loss_function(outputs, labels.to(device))\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "  \n",
        "          # print statistics\n",
        "          # running_loss += loss.item()\n",
        "          if i % 100 == 0:    # i=batch=第几个batch_size\n",
        "                loss, current = loss.item(), i * len(img_g)\n",
        "                print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "                niter = epoch * len(train_dataloader) + i\n",
        "                writer.add_scalars('Train',\n",
        "                                   {'train_loss': loss}, niter)\n",
        "                writer.add_scalars('Train',\n",
        "                                   {'learning_rate': optimizer.param_groups[0]['lr']}, niter)\n",
        "            # print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 10:.3f}')\n",
        "            \n",
        "\n",
        "    val_best_correct = 0\n",
        "    val_before_correct = 0\n",
        "    test_best_correct = 0\n",
        "    test_before_correct = 0\n",
        "    for t in range(epochs):\n",
        "        print(f\"Epoch {t + 1}\\n-------------------------------\")\n",
        "        train_loop(t, train_dataloader, model, loss_fn, base_optimizer)\n",
        "        val_best_correct = model_test(t, val_dataloader, model, val_best_correct)\n",
        "        test_best_correct = model_test(t, test_dataloader, model, test_best_correct)\n",
        "        scheduler_3.step()\n",
        "        if val_best_correct > 0.090 and val_best_correct > val_before_correct:\n",
        "            torch.save(model, './val_model/model_'+str(val_best_correct)+'.pth')\n",
        "            print('save_model: ', val_best_correct)\n",
        "\n",
        "        if test_best_correct > 0.090 and test_best_correct > test_before_correct:\n",
        "            torch.save(model, './test_model/model_'+str(test_best_correct)+'.pth')\n",
        "            print('save_model: ', test_best_correct)\n",
        "        val_before_correct = val_best_correct\n",
        "        test_before_correct = test_best_correct\n",
        "        print('done, val_best_score: ', val_best_correct)\n",
        "        print('done, test_best_score: ', test_best_correct)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ijnGvYS2I2ai"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "faz1aIjXHdGS"
      },
      "outputs": [],
      "source": [
        "-------------------------------------------------------------------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GoITkqPPFOcZ"
      },
      "source": [
        "#修剪"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "DnMIkeel_s9Z"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.utils.prune as prune\n",
        "import torchvision\n",
        "import random\n",
        "import sys\n",
        "import torch_pruning\n",
        "sys.path.append(\"/content/Model-Compression\")\n",
        "\n",
        "from model import *\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "random.seed(777)  #每次生成的随机数一样\n",
        "torch.manual_seed(777)\n",
        "if device == 'cuda':\n",
        "    torch.cuda.manual_seed(777)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-nCRIkCh9sPB"
      },
      "outputs": [],
      "source": [
        "learning_rate = 1e-4\n",
        "batch_size = 64\n",
        "epoch = 20"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101,
          "referenced_widgets": [
            "ca79308ea22148319a243830ebf02a0b",
            "752fdf8c763a4103a9c0f8c5d41e6b00",
            "9025e790f27441269399eb25a2f999c8",
            "5077b94b76834d01825b24e6771eb419",
            "ffa517ba87f3439e83af278e3ec60390",
            "16b3749fcb1247728c2f9645a6986f19",
            "9ed1d0abf9e343d8a05635963b5f9385",
            "d941bb465b6d4aadbad1d4ca9a5ee938",
            "11e12955346044a1b66b9a397155c90f",
            "5956c10bcf224746869cc95a8a0d5639",
            "a2fd82887191494bb7ea3d17a68d4a45"
          ]
        },
        "id": "-7AU0tqJ9u6A",
        "outputId": "34795920-c9b2-4710-c70d-572b97740eab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz to ./CIFAR100/cifar-100-python.tar.gz\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ca79308ea22148319a243830ebf02a0b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/169001437 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./CIFAR100/cifar-100-python.tar.gz to ./CIFAR100\n",
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "root = \"/content/12kDriveEnd_img0.2/train\"\n",
        "transform = torchvision.transforms.ToTensor()\n",
        "data_train = torchvision.datasets.CIFAR100(root, train = True, transform = transform, download = True)\n",
        "data_test = torchvision.datasets.CIFAR100(root, train = False, transform = transform, download = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JDiiMsaG9xyN"
      },
      "outputs": [],
      "source": [
        "train_loader = torch.utils.data.DataLoader(dataset=data_train, batch_size=batch_size, shuffle=True, drop_last=True)\n",
        "test_loader = torch.utils.data.DataLoader(dataset=data_test, batch_size=batch_size, shuffle=False, drop_last=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mZKxU5XD9zxl"
      },
      "outputs": [],
      "source": [
        "model = ResNet(Bottleneck, [3, 4, 6, 3], num_classes=100).to(device)\n",
        "checkpoint = torch.load(\"/content/Model-Compression/model.pth\")\n",
        "model.load_state_dict(checkpoint)\n",
        "criterion = torch.nn.CrossEntropyLoss().to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "54RbxQfz96yE"
      },
      "source": [
        "Code for Training and Eval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dUMWbU-I91-k"
      },
      "outputs": [],
      "source": [
        "def train_one_epoch(print_result = False):\n",
        "    model.train()\n",
        "    loss_sum = 0.0\n",
        "    accuracy_sum = 0.0\n",
        "    length = 0\n",
        "\n",
        "    for X, Y in train_loader:\n",
        "        X = X.to(device)\n",
        "        Y = Y.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        pred = model(X)\n",
        "        loss = criterion(pred, Y)\n",
        "        pred_idx = torch.argmax(pred, 1)\n",
        "        loss_sum += loss.item()\n",
        "        accuracy_sum += torch.sum((pred_idx == Y).float()).item()\n",
        "        length += X.size(0)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    if(print_result):\n",
        "        print(\"loss :\", loss_sum / length)\n",
        "        print(\"accuracy:\", accuracy_sum / length)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1LiX9eUb9-QZ"
      },
      "outputs": [],
      "source": [
        "def eval():\n",
        "    with torch.no_grad():\n",
        "        model.eval()\n",
        "        loss_sum = 0.0\n",
        "        accuracy_sum = 0.0\n",
        "        length = 0\n",
        "\n",
        "        for X, Y in train_loader:\n",
        "            X = X.to(device)\n",
        "            Y = Y.to(device)\n",
        "\n",
        "            pred = model(X)\n",
        "            loss = criterion(pred, Y)\n",
        "            pred_idx = torch.argmax(pred, 1)\n",
        "            loss_sum += loss.item()\n",
        "            accuracy_sum += torch.sum((pred_idx == Y).float()).item()\n",
        "            length += X.size(0)\n",
        "            \n",
        "        print(\"loss :\", loss_sum / length)\n",
        "        print(\"accuracy:\", accuracy_sum / length)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qO0AFIV8-BDk"
      },
      "outputs": [],
      "source": [
        "def train():\n",
        "    for i in range(epoch):\n",
        "        print(\"EPOCH[\" + str(i + 1) + \"]\")\n",
        "        print(\"==== train ====\")\n",
        "        train_one_epoch(print_result=True)\n",
        "        \n",
        "        print(\"==== eval ====\")\n",
        "        eval()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aNKVtey8-DD_"
      },
      "source": [
        "##Code for Pruning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "hz6yFfRL-TSa",
        "outputId": "188e0122-f176-4c49-8bbd-0ea198faf1f0"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "' code for pruning specific filters\\n\\nDG = torch_pruning.DependencyGraph()\\nDG.build_dependency(model, example_inputs=torch.randn(1, 3, 32, 32).to(\"cuda\"))\\npruning_idxs = [0, 1, 2, 3, 4] # remove filter 0 ~ 4\\npruning_group = DG.get_pruning_group(model.conv1, torch_pruning.prune_conv_out_channels, idxs = pruning_idxs)\\nprint(pruning_group)\\nif DG.check_pruning_group(pruning_group):\\n    pruning_group.exec()\\n'"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\"\"\" code for pruning specific filters\n",
        "\n",
        "DG = torch_pruning.DependencyGraph()\n",
        "DG.build_dependency(model, example_inputs=torch.randn(1, 3, 32, 32).to(\"cuda\"))\n",
        "pruning_idxs = [0, 1, 2, 3, 4] # remove filter 0 ~ 4\n",
        "pruning_group = DG.get_pruning_group(model.conv1, torch_pruning.prune_conv_out_channels, idxs = pruning_idxs)\n",
        "print(pruning_group)\n",
        "if DG.check_pruning_group(pruning_group):\n",
        "    pruning_group.exec()\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "akPsngOobwXe"
      },
      "source": [
        "##自己的"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 436
        },
        "id": "ySUJMKwe-E0x",
        "outputId": "06e37193-de82-4381-d167-71d3ee53234c"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch_pruning/dependency.py\u001b[0m in \u001b[0;36m_trace\u001b[0;34m(self, model, example_inputs, forward_fn, output_transform)\u001b[0m\n\u001b[1;32m    510\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 511\u001b[0;31m                 \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mexample_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    512\u001b[0m             \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: forward() takes 3 positional arguments but 65 were given",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-e4a01b01a1b6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m#ch_sparsity = 0.5 means 50% of the filters are pruned\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m pruner = torch_pruning.pruner.MagnitudePruner(\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mexample_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexample_inputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch_pruning/pruner/algorithms/metapruner.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model, example_inputs, importance, global_pruning, ch_sparsity, ch_sparsity_dict, max_ch_sparsity, iterative_steps, iterative_sparsity_scheduler, ignored_layers, round_to, channel_groups, customized_pruners, unwrapped_parameters, root_module_types, output_transform)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0;31m# Build dependency graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m         self.DG = dependency.DependencyGraph().build_dependency(\n\u001b[0m\u001b[1;32m     63\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m             \u001b[0mexample_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexample_inputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch_pruning/dependency.py\u001b[0m in \u001b[0;36mbuild_dependency\u001b[0;34m(self, model, example_inputs, forward_fn, output_transform, unwrapped_parameters, customized_pruners, verbose)\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m         \u001b[0;31m# Build computational graph by tracing.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 254\u001b[0;31m         self.module2node = self._trace(\n\u001b[0m\u001b[1;32m    255\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexample_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforward_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_transform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_transform\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch_pruning/dependency.py\u001b[0m in \u001b[0;36m_trace\u001b[0;34m(self, model, example_inputs, forward_fn, output_transform)\u001b[0m\n\u001b[1;32m    511\u001b[0m                 \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mexample_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    512\u001b[0m             \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 513\u001b[0;31m                 \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexample_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    514\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    515\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: forward() missing 1 required positional argument: 'img_g'"
          ]
        }
      ],
      "source": [
        "# code for pruning whole model\n",
        "\n",
        "# example_inputs = torch.randn(1, 3, 32, 32).to(\"cuda\")\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "example_inputs = torch.randn(64, 1, 200, 200)\n",
        "model=final_model_2.to(device)\n",
        "imp = torch_pruning.importance.MagnitudeImportance(p = 2)  # importance criterion for parameter selections\n",
        "ignored_layers = []  # ignore some layers that should not be pruned, e.g., the final classifier layer.\n",
        "for m in model.modules():\n",
        "    if isinstance(m, torch.nn.Linear) and m.out_features == 100:\n",
        "        ignored_layers.append(m)  # DO NOT prune the final classifier!\n",
        "\n",
        "#ch_sparsity = 0.5 means 50% of the filters are pruned\n",
        "pruner = torch_pruning.pruner.MagnitudePruner(\n",
        "    model=model, \n",
        "    example_inputs=example_inputs, \n",
        "    importance=imp, # importance criterion for parameter selection\n",
        "    iterative_steps=5, # You can prune your model to the target sparsity iteratively.\n",
        "    ch_sparsity=0.7, # remove 50% channels, ResNet18 = {64, 128, 256, 512} => ResNet18_Half = {32, 64, 128, 256}\n",
        "    ignored_layers=ignored_layers\n",
        "    )\n",
        "\n",
        "base_macs, base_nparams = torch_pruning.utils.count_ops_and_params(model=model, example_inputs=example_inputs)  #mac就是flops\n",
        "print(\"basemacs :\", base_macs, \"basenparams :\", base_nparams)\n",
        "for i in range(5):  #5=iterative_steps\n",
        "\n",
        "    pruner.step()  # the pruner.step will remove some channels from the model with least importance\n",
        "\n",
        "    macs, nparams = torch_pruning.utils.count_ops_and_params(model=model, example_inputs=example_inputs)#  Do whatever you like here, such as fintuning\n",
        "    print(\"iter [\", i, \"] macs :\", macs, \"nparams :\", nparams)\n",
        "\n",
        "    #finetune\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)\n",
        "\n",
        "    print(\"==== train ====\")\n",
        "    for j in range(10):\n",
        "        train_one_epoch(print_result = True)\n",
        "    \n",
        "    print(\"==== eval ====\")\n",
        "    eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 436
        },
        "id": "Qb_SmnEiZMRe",
        "outputId": "925ece65-1e76-4434-a4cf-da1cd513bc9a"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch_pruning/dependency.py\u001b[0m in \u001b[0;36m_trace\u001b[0;34m(self, model, example_inputs, forward_fn, output_transform)\u001b[0m\n\u001b[1;32m    510\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 511\u001b[0;31m                 \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mexample_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    512\u001b[0m             \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: forward() takes 3 positional arguments but 65 were given",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-df4f2327326e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m#ch_sparsity = 0.5 means 50% of the filters are pruned\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m pruner = torch_pruning.pruner.MagnitudePruner(\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mexample_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexample_inputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch_pruning/pruner/algorithms/metapruner.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model, example_inputs, importance, global_pruning, ch_sparsity, ch_sparsity_dict, max_ch_sparsity, iterative_steps, iterative_sparsity_scheduler, ignored_layers, round_to, channel_groups, customized_pruners, unwrapped_parameters, root_module_types, output_transform)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0;31m# Build dependency graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m         self.DG = dependency.DependencyGraph().build_dependency(\n\u001b[0m\u001b[1;32m     63\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m             \u001b[0mexample_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexample_inputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch_pruning/dependency.py\u001b[0m in \u001b[0;36mbuild_dependency\u001b[0;34m(self, model, example_inputs, forward_fn, output_transform, unwrapped_parameters, customized_pruners, verbose)\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m         \u001b[0;31m# Build computational graph by tracing.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 254\u001b[0;31m         self.module2node = self._trace(\n\u001b[0m\u001b[1;32m    255\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexample_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforward_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_transform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_transform\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch_pruning/dependency.py\u001b[0m in \u001b[0;36m_trace\u001b[0;34m(self, model, example_inputs, forward_fn, output_transform)\u001b[0m\n\u001b[1;32m    511\u001b[0m                 \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mexample_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    512\u001b[0m             \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 513\u001b[0;31m                 \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexample_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    514\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    515\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: forward() missing 1 required positional argument: 'img_g'"
          ]
        }
      ],
      "source": [
        "# code for pruning whole model\n",
        "\n",
        "example_inputs = torch.randn(64, 1, 200, 200).to(\"cuda\")\n",
        "\n",
        "imp = torch_pruning.importance.MagnitudeImportance(p = 2)  # importance criterion for parameter selections\n",
        "ignored_layers = []  # ignore some layers that should not be pruned, e.g., the final classifier layer.\n",
        "for m in model.modules():\n",
        "    if isinstance(m, torch.nn.Linear) and m.out_features == 100:\n",
        "        ignored_layers.append(m)  # DO NOT prune the final classifier!\n",
        "\n",
        "#ch_sparsity = 0.5 means 50% of the filters are pruned\n",
        "pruner = torch_pruning.pruner.MagnitudePruner(\n",
        "    model=model, \n",
        "    example_inputs=example_inputs, \n",
        "    importance=imp, # importance criterion for parameter selection\n",
        "    iterative_steps=5, # You can prune your model to the target sparsity iteratively.\n",
        "    ch_sparsity=0.7, # remove 50% channels, ResNet18 = {64, 128, 256, 512} => ResNet18_Half = {32, 64, 128, 256}\n",
        "    ignored_layers=ignored_layers\n",
        "    )\n",
        "\n",
        "base_macs, base_nparams = torch_pruning.utils.count_ops_and_params(model=model, example_inputs=example_inputs)\n",
        "print(\"basemacs :\", base_macs, \"basenparams :\", base_nparams)\n",
        "for i in range(5):  #5=iterative_steps\n",
        "\n",
        "    pruner.step()  # the pruner.step will remove some channels from the model with least importance\n",
        "\n",
        "    macs, nparams = torch_pruning.utils.count_ops_and_params(model=model, example_inputs=example_inputs)#  Do whatever you like here, such as fintuning\n",
        "    print(\"iter [\", i, \"] macs :\", macs, \"nparams :\", nparams)\n",
        "\n",
        "    #finetune\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)\n",
        "\n",
        "    print(\"==== train ====\")\n",
        "    for j in range(10):\n",
        "        train_one_epoch(print_result = True)\n",
        "    \n",
        "    print(\"==== eval ====\")\n",
        "    eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1WPJh0-8-XET"
      },
      "outputs": [],
      "source": [
        "eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QjC7Evms-dpV"
      },
      "outputs": [],
      "source": [
        "# torch.save(model, \"/content/Model-Compression/pruned.pth\")\n",
        "# model = torch.load(\"/content/Model-Compression/pruned.pth\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOWJE/c9/dYcxgdF2PVUGOY",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "11e12955346044a1b66b9a397155c90f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "16b3749fcb1247728c2f9645a6986f19": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5077b94b76834d01825b24e6771eb419": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5956c10bcf224746869cc95a8a0d5639",
            "placeholder": "​",
            "style": "IPY_MODEL_a2fd82887191494bb7ea3d17a68d4a45",
            "value": " 169001437/169001437 [00:01&lt;00:00, 90455136.20it/s]"
          }
        },
        "5956c10bcf224746869cc95a8a0d5639": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "752fdf8c763a4103a9c0f8c5d41e6b00": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_16b3749fcb1247728c2f9645a6986f19",
            "placeholder": "​",
            "style": "IPY_MODEL_9ed1d0abf9e343d8a05635963b5f9385",
            "value": "100%"
          }
        },
        "9025e790f27441269399eb25a2f999c8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d941bb465b6d4aadbad1d4ca9a5ee938",
            "max": 169001437,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_11e12955346044a1b66b9a397155c90f",
            "value": 169001437
          }
        },
        "9ed1d0abf9e343d8a05635963b5f9385": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a2fd82887191494bb7ea3d17a68d4a45": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ca79308ea22148319a243830ebf02a0b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_752fdf8c763a4103a9c0f8c5d41e6b00",
              "IPY_MODEL_9025e790f27441269399eb25a2f999c8",
              "IPY_MODEL_5077b94b76834d01825b24e6771eb419"
            ],
            "layout": "IPY_MODEL_ffa517ba87f3439e83af278e3ec60390"
          }
        },
        "d941bb465b6d4aadbad1d4ca9a5ee938": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ffa517ba87f3439e83af278e3ec60390": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "224c53c7cd4e45828f646cbd50010808": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7050dc771e8a434ca0bcd5bea422540a",
              "IPY_MODEL_9fb954230742487694688ecb70d2fde4",
              "IPY_MODEL_c2dd901524f445be86e615187eacc0be"
            ],
            "layout": "IPY_MODEL_00ba53c0c0d64f9cbd543c87aa97351a"
          }
        },
        "7050dc771e8a434ca0bcd5bea422540a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_853ac712ee22480689fd3d18df40904d",
            "placeholder": "​",
            "style": "IPY_MODEL_29d6adf067f44a5d8d5e6855d58e43ed",
            "value": "100%"
          }
        },
        "9fb954230742487694688ecb70d2fde4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2564096686ef4017b379a83321f9b129",
            "max": 169001437,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0e831f6e7618438db3921f3b5868fc45",
            "value": 169001437
          }
        },
        "c2dd901524f445be86e615187eacc0be": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_40373bc809854fbea79abdce6c4329bd",
            "placeholder": "​",
            "style": "IPY_MODEL_a096d8f4f4fd41cda675521c045a88f5",
            "value": " 169001437/169001437 [00:05&lt;00:00, 32840420.30it/s]"
          }
        },
        "00ba53c0c0d64f9cbd543c87aa97351a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "853ac712ee22480689fd3d18df40904d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "29d6adf067f44a5d8d5e6855d58e43ed": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2564096686ef4017b379a83321f9b129": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0e831f6e7618438db3921f3b5868fc45": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "40373bc809854fbea79abdce6c4329bd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a096d8f4f4fd41cda675521c045a88f5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}